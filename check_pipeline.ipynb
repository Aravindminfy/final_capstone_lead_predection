{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08af86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598a1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31786bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 21:18:31,036 - INFO - Preprocessing pipeline loaded successfully\n",
      "2025-07-17 21:18:31,040 - INFO - Metadata loaded successfully\n",
      "2025-07-17 21:18:31,078 - INFO - Model loaded from key: 'model'\n",
      "2025-07-17 21:18:31,080 - INFO - Model type: <class 'xgboost.sklearn.XGBClassifier'>\n",
      "2025-07-17 21:18:31,374 - INFO - Preprocessing pipeline loaded successfully\n",
      "2025-07-17 21:18:31,376 - INFO - Metadata loaded successfully\n",
      "2025-07-17 21:18:31,386 - INFO - Model loaded from key: 'model'\n",
      "2025-07-17 21:18:31,387 - INFO - Model type: <class 'xgboost.sklearn.XGBClassifier'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: XGBClassifier\n",
      "Total features: 179\n",
      "Prediction Summary:\n",
      "  - Total samples: 5\n",
      "  - Predictions: {0: 5}\n",
      "  - Accuracy: 60.00%\n",
      "Prediction pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LeadConversionPipeline:\n",
    "    \"\"\"\n",
    "    A pipeline for lead conversion prediction.\n",
    "\n",
    "    This class provides functionality to:\n",
    "    - Load trained preprocessing pipeline and model\n",
    "    - Clean and preprocess incoming data\n",
    "    - Align features as per the training phase\n",
    "    - Perform predictions\n",
    "    - Save prediction results to a file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pipeline_path: str, metadata_path: str, model_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline by loading all required components.\n",
    "\n",
    "        Args:\n",
    "            pipeline_path (str): Path to the preprocessing pipeline (.pkl file)\n",
    "            metadata_path (str): Path to the metadata file (.pkl)\n",
    "            model_path (str): Path to the trained model (.pkl)\n",
    "        \"\"\"\n",
    "        self.pipeline_path = pipeline_path\n",
    "        self.metadata_path = metadata_path\n",
    "        self.model_path = model_path\n",
    "\n",
    "        self.preprocessor = None\n",
    "        self.metadata = None\n",
    "        self.model = None\n",
    "        self.model_data = None\n",
    "\n",
    "        self._load_components()\n",
    "\n",
    "    def _load_components(self) -> None:\n",
    "        \"\"\"Loads the preprocessing pipeline, metadata, and trained model from disk.\"\"\"\n",
    "        try:\n",
    "            with open(self.pipeline_path, 'rb') as f:\n",
    "                self.preprocessor = pickle.load(f)\n",
    "            logger.info(\"Preprocessing pipeline loaded successfully\")\n",
    "\n",
    "            with open(self.metadata_path, 'rb') as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "            logger.info(\"Metadata loaded successfully\")\n",
    "\n",
    "            with open(self.model_path, 'rb') as f:\n",
    "                self.model_data = pickle.load(f)\n",
    "\n",
    "            if isinstance(self.model_data, dict):\n",
    "                for key in ['best_estimator_', 'model', 'estimator', 'best_model', 'classifier']:\n",
    "                    if key in self.model_data:\n",
    "                        self.model = self.model_data[key]\n",
    "                        logger.info(f\"Model loaded from key: '{key}'\")\n",
    "                        break\n",
    "                if self.model is None:\n",
    "                    for key, value in self.model_data.items():\n",
    "                        if hasattr(value, 'predict'):\n",
    "                            self.model = value\n",
    "                            break\n",
    "            else:\n",
    "                self.model = self.model_data\n",
    "\n",
    "            if not hasattr(self.model, 'predict'):\n",
    "                raise ValueError(\"Loaded object is not a valid model\")\n",
    "\n",
    "            logger.info(f\"Model type: {type(self.model)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading components: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _clean_data_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Cleans the input DataFrame to correct data types and missing values.\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        numerical_cols = self.metadata['numerical_features']\n",
    "        categorical_cols = self.metadata['categorical_features']\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col].astype(str), errors='coerce')\n",
    "                if df_clean[col].isna().sum() > 0:\n",
    "                    median_val = df_clean[col].median()\n",
    "                    df_clean[col].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].astype(str).replace(['nan', 'None', 'null'], 'Unknown').str.strip()\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "    def _encode_binary_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Encodes binary categorical values into numerical format.\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        binary_cols = self.metadata['binary_features']\n",
    "\n",
    "        for col in binary_cols:\n",
    "            if col in df_encoded.columns:\n",
    "                df_encoded[col] = df_encoded[col].astype(str).str.strip().str.lower()\n",
    "                binary_mapping = {\n",
    "                    'yes': 1, 'y': 1, '1': 1, 'true': 1, 'on': 1,\n",
    "                    'no': 0, 'n': 0, '0': 0, 'false': 0, 'off': 0,\n",
    "                    'unknown': 0, 'nan': 0, 'none': 0\n",
    "                }\n",
    "                df_encoded[f'{col}_encoded'] = df_encoded[col].map(binary_mapping).fillna(0).astype(int)\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    def _align_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aligns feature columns to match those used in model training.\"\"\"\n",
    "        if 'feature_names' in self.model_data:\n",
    "            expected_features = self.model_data['feature_names']\n",
    "        elif hasattr(self.model, 'feature_names_in_'):\n",
    "            expected_features = self.model.feature_names_in_\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "        X_aligned = pd.DataFrame(0, index=X.index, columns=expected_features)\n",
    "\n",
    "        for col in X.columns:\n",
    "            if col in expected_features:\n",
    "                X_aligned[col] = X[col]\n",
    "\n",
    "        return X_aligned\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"Runs the entire preprocessing pipeline on the input data.\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        actual_labels = None\n",
    "\n",
    "        if 'Converted' in df_processed.columns:\n",
    "            actual_labels = df_processed['Converted']\n",
    "            df_processed.drop(columns=['Converted'], inplace=True)\n",
    "\n",
    "        df_processed = self._clean_data_types(df_processed)\n",
    "        df_processed = self._encode_binary_features(df_processed)\n",
    "\n",
    "        numerical_cols = self.metadata['numerical_features']\n",
    "        categorical_cols = self.metadata['categorical_features']\n",
    "        binary_cols = self.metadata['binary_features']\n",
    "\n",
    "        available_num_cols = [col for col in numerical_cols if col in df_processed.columns]\n",
    "        available_cat_cols = [col for col in categorical_cols if col in df_processed.columns]\n",
    "\n",
    "        if available_num_cols or available_cat_cols:\n",
    "            X_sample = df_processed[available_num_cols + available_cat_cols]\n",
    "            X_transformed = self.preprocessor.transform(X_sample)\n",
    "\n",
    "            try:\n",
    "                feature_names = self.preprocessor.get_feature_names_out()\n",
    "            except:\n",
    "                feature_names = [f\"feature_{i}\" for i in range(X_transformed.shape[1])]\n",
    "\n",
    "            X_final = pd.DataFrame(X_transformed, columns=feature_names, index=df_processed.index)\n",
    "\n",
    "            for col in [f\"{col}_encoded\" for col in binary_cols if col in df.columns]:\n",
    "                if col in df_processed.columns:\n",
    "                    X_final[col] = df_processed[col].values\n",
    "\n",
    "            X_final = self._align_features(X_final)\n",
    "            return X_final, actual_labels\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"No valid numerical or categorical columns found for preprocessing\")\n",
    "\n",
    "    def predict(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generates predictions for the given input data.\"\"\"\n",
    "        X_final, actual_labels = self.preprocess_data(df)\n",
    "        predictions = self.model.predict(X_final)\n",
    "\n",
    "        try:\n",
    "            probabilities = self.model.predict_proba(X_final)\n",
    "        except:\n",
    "            probabilities = None\n",
    "\n",
    "        results = {\n",
    "            'predictions': predictions,\n",
    "            'probabilities': probabilities,\n",
    "            'actual_labels': actual_labels,\n",
    "            'input_data': df.copy()\n",
    "        }\n",
    "\n",
    "        if actual_labels is not None:\n",
    "            accuracy = (predictions == actual_labels).mean() * 100\n",
    "            results['accuracy'] = accuracy\n",
    "\n",
    "        return results\n",
    "\n",
    "    def predict_from_file(self, file_path: str, output_path: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Reads data from a CSV file, makes predictions, and optionally saves results to CSV.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        results = self.predict(df)\n",
    "\n",
    "        results_df = results['input_data'].copy()\n",
    "        results_df['Predicted_Converted'] = results['predictions']\n",
    "\n",
    "        if results['probabilities'] is not None:\n",
    "            prob_df = pd.DataFrame(results['probabilities'],\n",
    "                                   columns=[f'Prob_Class_{i}' for i in range(results['probabilities'].shape[1])],\n",
    "                                   index=results_df.index)\n",
    "            results_df = pd.concat([results_df, prob_df], axis=1)\n",
    "\n",
    "        if results['actual_labels'] is not None:\n",
    "            results_df['Actual_Converted'] = results['actual_labels']\n",
    "            results_df['Prediction_Correct'] = (results_df['Predicted_Converted'] == results_df['Actual_Converted'])\n",
    "\n",
    "        if output_path:\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "\n",
    "        results['results_df'] = results_df\n",
    "        return results\n",
    "\n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Returns metadata and details about the loaded model.\"\"\"\n",
    "        info = {\n",
    "            'model_type': type(self.model).__name__,\n",
    "            'numerical_features': self.metadata['numerical_features'],\n",
    "            'categorical_features': self.metadata['categorical_features'],\n",
    "            'binary_features': self.metadata['binary_features']\n",
    "        }\n",
    "\n",
    "        if 'feature_names' in self.model_data:\n",
    "            info['total_features'] = len(self.model_data['feature_names'])\n",
    "            info['feature_names'] = self.model_data['feature_names']\n",
    "\n",
    "        return info\n",
    "\n",
    "# Utility Functions\n",
    "def create_pipeline(pipeline_path: str, metadata_path: str, model_path: str) -> LeadConversionPipeline:\n",
    "    \"\"\"Instantiates the LeadConversionPipeline class.\"\"\"\n",
    "    return LeadConversionPipeline(pipeline_path, metadata_path, model_path)\n",
    "\n",
    "def predict_from_csv(pipeline_path: str, metadata_path: str, model_path: str,\n",
    "                     input_csv: str, output_csv: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Convenience function to predict using input CSV file.\"\"\"\n",
    "    pipeline = create_pipeline(pipeline_path, metadata_path, model_path)\n",
    "    return pipeline.predict_from_file(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    PIPELINE_PATH = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\preprocess\\preprocessed_output\\processed\\pipeline_model.pkl\"\n",
    "    METADATA_PATH = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\preprocess\\preprocessed_output\\processed\\metadata.pkl\"\n",
    "    MODEL_PATH = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\modal_development\\model\\best_model_xgboost.pkl\"\n",
    "    INPUT_CSV = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\raw_data\\check.csv\"\n",
    "    OUTPUT_CSV = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\prediction_results.csv\"\n",
    "\n",
    "    # Option 1: Use convenience function\n",
    "    results = predict_from_csv(PIPELINE_PATH, METADATA_PATH, MODEL_PATH, INPUT_CSV, OUTPUT_CSV)\n",
    "\n",
    "    # Option 2: Use the class directly\n",
    "    pipeline = create_pipeline(PIPELINE_PATH, METADATA_PATH, MODEL_PATH)\n",
    "    model_info = pipeline.get_model_info()\n",
    "\n",
    "    print(f\"Model type: {model_info['model_type']}\")\n",
    "    print(f\"Total features: {model_info.get('total_features', 'Unknown')}\")\n",
    "\n",
    "    results = pipeline.predict_from_file(INPUT_CSV, OUTPUT_CSV)\n",
    "    print(\"Prediction Summary:\")\n",
    "    print(f\"  - Total samples: {len(results['predictions'])}\")\n",
    "    print(f\"  - Predictions: {pd.Series(results['predictions']).value_counts().to_dict()}\")\n",
    "    if 'accuracy' in results:\n",
    "        print(f\"  - Accuracy: {results['accuracy']:.2f}%\")\n",
    "\n",
    "    print(\"Prediction pipeline completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388700b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
