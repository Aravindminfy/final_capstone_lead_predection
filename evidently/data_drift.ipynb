{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0483a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive drift detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/18 13:41:44 INFO mlflow.tracking.fluent: Experiment with name 'drift_detection_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference data shape: (9240, 37)\n",
      "New data shape: (5, 37)\n",
      "Numerical features: ['Lead Number', 'Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit', 'Asymmetrique Activity Score', 'Asymmetrique Profile Score']\n",
      "Categorical features: ['Prospect ID', 'Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Last Activity', 'Country', 'Specialization', 'How did you hear about X Education', 'What is your current occupation', 'What matters most to you in choosing a course', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', 'Tags', 'Lead Quality', 'Update me on Supply Chain Content', 'Get updates on DM Content', 'Lead Profile', 'City', 'Asymmetrique Activity Index', 'Asymmetrique Profile Index', 'I agree to pay the amount through cheque', 'A free copy of Mastering The Interview', 'Last Notable Activity']\n",
      "Logging basic statistics...\n",
      "Generating drift report using psi method...\n",
      "Evidently report failed for drift - psi: Stattest psi isn't applicable to feature of type text. Available feature types: [<ColumnType.Categorical: 'cat'>, <ColumnType.Numerical: 'num'>]\n",
      "Evidently report failed for summary - psi: 2 validation errors for ByLabelCountValue\n",
      "counts\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "shares\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "Drift detection using psi: NOT DETECTED\n",
      "Generating drift report using ks method...\n",
      "Evidently report failed for drift - ks: Stattest ks isn't applicable to feature of type cat. Available feature types: [<ColumnType.Numerical: 'num'>]\n",
      "Evidently report failed for summary - ks: 2 validation errors for ByLabelCountValue\n",
      "counts\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "shares\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "Drift detection using ks: NOT DETECTED\n",
      "Generating drift report using chisquare method...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference data shape: (9240, 37)\n",
      "New data shape: (5, 37)\n",
      "Numerical features: ['Lead Number', 'Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit', 'Asymmetrique Activity Score', 'Asymmetrique Profile Score']\n",
      "Categorical features: ['Prospect ID', 'Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Last Activity', 'Country', 'Specialization', 'How did you hear about X Education', 'What is your current occupation', 'What matters most to you in choosing a course', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', 'Tags', 'Lead Quality', 'Update me on Supply Chain Content', 'Get updates on DM Content', 'Lead Profile', 'City', 'Asymmetrique Activity Index', 'Asymmetrique Profile Index', 'I agree to pay the amount through cheque', 'A free copy of Mastering The Interview', 'Last Notable Activity']\n",
      "Logging basic statistics...\n",
      "Generating drift report using psi method...\n",
      "Evidently report failed for drift - psi: Stattest psi isn't applicable to feature of type text. Available feature types: [<ColumnType.Categorical: 'cat'>, <ColumnType.Numerical: 'num'>]\n",
      "Evidently report failed for summary - psi: 2 validation errors for ByLabelCountValue\n",
      "counts\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "shares\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "Drift detection using psi: NOT DETECTED\n",
      "Generating drift report using ks method...\n",
      "Evidently report failed for drift - ks: Stattest ks isn't applicable to feature of type cat. Available feature types: [<ColumnType.Numerical: 'num'>]\n",
      "Evidently report failed for summary - ks: 2 validation errors for ByLabelCountValue\n",
      "counts\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "shares\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "Drift detection using ks: NOT DETECTED\n",
      "Generating drift report using chisquare method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "c:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:7400: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently report failed for drift - chisquare: Stattest chisquare isn't applicable to feature of type text. Available feature types: [<ColumnType.Categorical: 'cat'>]\n",
      "Evidently report failed for summary - chisquare: 2 validation errors for ByLabelCountValue\n",
      "counts\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "shares\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "Drift detection using chisquare: NOT DETECTED\n",
      "Generating drift report using wasserstein method...\n",
      "Evidently report failed for drift - wasserstein: Stattest wasserstein isn't applicable to feature of type cat. Available feature types: [<ColumnType.Numerical: 'num'>]\n",
      "Evidently report failed for summary - wasserstein: 2 validation errors for ByLabelCountValue\n",
      "counts\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "shares\n",
      "  type <class 'numpy.float64'> not supported as Label (type=value_error)\n",
      "Drift detection using wasserstein: NOT DETECTED\n",
      "\n",
      "Drift Detection Summary:\n",
      "Overall drift detected: NO\n",
      "Individual method results: {'psi': False, 'ks': False, 'chisquare': False, 'wasserstein': False}\n",
      "Reports saved and logged to MLflow\n",
      "Drift flag saved to: C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\evidently\\drift_flag.txt\n",
      "ðŸƒ View run sincere-newt-376 at: http://localhost:5000/#/experiments/627708234962460646/runs/9e8c136e366e4f4ea338a4dd6ea3a31a\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/627708234962460646\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset, DataSummaryPreset\n",
    "\n",
    "# === Configuration ===\n",
    "# File Paths\n",
    "REFERENCE_DATA_PATH = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\raw_data\\Lead Scoring.csv\"\n",
    "NEW_DATA_PATH = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\raw_data\\check.csv\"\n",
    "DRIFT_REPORT_PATH = \"drift_report.html\"\n",
    "SUMMARY_REPORT_PATH = \"summary_report.html\"\n",
    "DRIFT_FLAG_PATH = r\"C:\\Users\\Minfy.DESKTOP-3E50D5N\\Desktop\\final_capstone\\evidently\\drift_flag.txt\"\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"  # Adjust as needed\n",
    "MLFLOW_EXPERIMENT_NAME = \"drift_detection_experiment\"\n",
    "\n",
    "# Drift Detection Parameters\n",
    "DRIFT_THRESHOLD = 0.3  # PSI threshold for drift detection\n",
    "DRIFT_METHODS = ['psi', 'ks', 'chisquare', 'wasserstein']  # Multiple drift detection methods\n",
    "NUMERICAL_FEATURES = []  # Will be auto-detected\n",
    "CATEGORICAL_FEATURES = []  # Will be auto-detected\n",
    "\n",
    "# === Utility Functions ===\n",
    "def clean_metric_name(name):\n",
    "    \"\"\"Clean metric names to be MLflow-friendly by removing unsupported characters.\"\"\"\n",
    "    return re.sub(r\"[^\\w\\-/\\. ]\", \"_\", name)\n",
    "\n",
    "def setup_mlflow():\n",
    "    \"\"\"Initialize MLflow tracking.\"\"\"\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "    return mlflow.start_run()\n",
    "\n",
    "def load_and_validate_data(reference_path, new_path):\n",
    "    \"\"\"Load and validate reference and new datasets.\"\"\"\n",
    "    try:\n",
    "        reference_df = pd.read_csv(reference_path)\n",
    "        new_df = pd.read_csv(new_path)\n",
    "        \n",
    "        print(f\"Reference data shape: {reference_df.shape}\")\n",
    "        print(f\"New data shape: {new_df.shape}\")\n",
    "        \n",
    "        # Log basic dataset info\n",
    "        mlflow.log_param(\"reference_data_rows\", reference_df.shape[0])\n",
    "        mlflow.log_param(\"reference_data_cols\", reference_df.shape[1])\n",
    "        mlflow.log_param(\"new_data_rows\", new_df.shape[0])\n",
    "        mlflow.log_param(\"new_data_cols\", new_df.shape[1])\n",
    "        \n",
    "        return reference_df, new_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def identify_feature_types(df):\n",
    "    \"\"\"Automatically identify numerical and categorical features.\"\"\"\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numerical features: {numerical_features}\")\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    \n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "def log_basic_statistics(reference_df, new_df, prefix=\"\"):\n",
    "    \"\"\"Log basic statistics for both datasets.\"\"\"\n",
    "    stats_metrics = {}\n",
    "    \n",
    "    # Reference dataset statistics\n",
    "    ref_stats = reference_df.describe()\n",
    "    for col in ref_stats.columns:\n",
    "        for stat in ref_stats.index:\n",
    "            metric_name = clean_metric_name(f\"{prefix}reference_{col}_{stat}\")\n",
    "            value = ref_stats.loc[stat, col]\n",
    "            if pd.notna(value):\n",
    "                stats_metrics[metric_name] = float(value)\n",
    "    \n",
    "    # New dataset statistics\n",
    "    new_stats = new_df.describe()\n",
    "    for col in new_stats.columns:\n",
    "        for stat in new_stats.index:\n",
    "            metric_name = clean_metric_name(f\"{prefix}new_{col}_{stat}\")\n",
    "            value = new_stats.loc[stat, col]\n",
    "            if pd.notna(value):\n",
    "                stats_metrics[metric_name] = float(value)\n",
    "    \n",
    "    # Log missing values\n",
    "    ref_missing = reference_df.isnull().sum()\n",
    "    new_missing = new_df.isnull().sum()\n",
    "    \n",
    "    for col in reference_df.columns:\n",
    "        stats_metrics[clean_metric_name(f\"{prefix}reference_{col}_missing_count\")] = int(ref_missing[col])\n",
    "        stats_metrics[clean_metric_name(f\"{prefix}reference_{col}_missing_percentage\")] = float(ref_missing[col] / len(reference_df) * 100)\n",
    "    \n",
    "    for col in new_df.columns:\n",
    "        stats_metrics[clean_metric_name(f\"{prefix}new_{col}_missing_count\")] = int(new_missing[col])\n",
    "        stats_metrics[clean_metric_name(f\"{prefix}new_{col}_missing_percentage\")] = float(new_missing[col] / len(new_df) * 100)\n",
    "    \n",
    "    # Log all statistics\n",
    "    for metric_name, value in stats_metrics.items():\n",
    "        mlflow.log_metric(metric_name, value)\n",
    "    \n",
    "    return stats_metrics\n",
    "\n",
    "def log_evidently_reports(reference_df, new_df, method='psi'):\n",
    "    \"\"\"\n",
    "    This function runs Evidently data drift and summary reports,\n",
    "    logs them to MLflow, and detects if drift exceeds threshold\n",
    "    \"\"\"\n",
    "    # Define types of reports to generate\n",
    "    report_configs = [\n",
    "        (\"drift\", DataDriftPreset(method=method)),  # Drift detection\n",
    "        (\"summary\", DataSummaryPreset())            # Summary report\n",
    "    ]\n",
    "    \n",
    "    drift_found = False  # Flag to track drift detection\n",
    "    \n",
    "    # Ensure common columns\n",
    "    common_cols = reference_df.columns.intersection(new_df.columns)\n",
    "    ref_df_clean = reference_df[common_cols].copy()\n",
    "    new_df_clean = new_df[common_cols].copy()\n",
    "    \n",
    "    for report_type, preset in report_configs:\n",
    "        report = Report([preset], include_tests=True)\n",
    "        \n",
    "        try:\n",
    "            # Run the Evidently report\n",
    "            result = report.run(reference_data=ref_df_clean, current_data=new_df_clean)\n",
    "            \n",
    "            # Save report as HTML and log to MLflow\n",
    "            html_path = f\"evidently_{report_type}_{method}.html\"\n",
    "            result.save_html(html_path)\n",
    "            mlflow.log_artifact(html_path)\n",
    "            \n",
    "            # Convert report to JSON to extract drift metrics\n",
    "            json_data = json.loads(result.json())\n",
    "            \n",
    "            # Loop through all reported metrics\n",
    "            for metric in json_data.get(\"metrics\", []):\n",
    "                metric_id = metric.get(\"metric_id\") or metric.get(\"metric\", \"\")\n",
    "                value = metric.get(\"value\", None)\n",
    "                \n",
    "                # If metric value is a dictionary (contains sub-metrics)\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_name, sub_val in value.items():\n",
    "                        if isinstance(sub_val, (int, float)):\n",
    "                            metric_name = clean_metric_name(f\"{method}_{report_type}_{metric_id}_{sub_name}\")\n",
    "                            mlflow.log_metric(metric_name, sub_val)\n",
    "                            # Check if drift is found\n",
    "                            if \"drift\" in sub_name.lower() and sub_val > DRIFT_THRESHOLD:\n",
    "                                drift_found = True\n",
    "                \n",
    "                # If metric is a single float or int\n",
    "                elif isinstance(value, (int, float)):\n",
    "                    metric_name = clean_metric_name(f\"{method}_{report_type}_{metric_id}\")\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "                    if \"drift\" in metric_name.lower() and value > DRIFT_THRESHOLD:\n",
    "                        drift_found = True\n",
    "                \n",
    "                # Special handling for column-wise drift\n",
    "                elif \"ValueDrift(column=\" in str(metric_id):\n",
    "                    try:\n",
    "                        col_name = str(metric_id).split(\"ValueDrift(column=\")[1].split(\",\")[0]\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            metric_name = clean_metric_name(f\"{method}_{report_type}_{col_name}_drift\")\n",
    "                            mlflow.log_metric(metric_name, value)\n",
    "                            if value > DRIFT_THRESHOLD:\n",
    "                                drift_found = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not parse drift metric: {metric_id} â€” {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Evidently report failed for {report_type} - {method}: {e}\")\n",
    "    \n",
    "    return drift_found\n",
    "\n",
    "def save_drift_flag(drift_detected, additional_info=None):\n",
    "    \"\"\"Save drift detection result to file.\"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(DRIFT_FLAG_PATH), exist_ok=True)\n",
    "        \n",
    "        drift_info = {\n",
    "            \"drift_detected\": drift_detected,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"drift_threshold\": DRIFT_THRESHOLD,\n",
    "            \"additional_info\": additional_info or {}\n",
    "        }\n",
    "        \n",
    "        with open(DRIFT_FLAG_PATH, \"w\") as f:\n",
    "            json.dump(drift_info, f, indent=2)\n",
    "        \n",
    "        print(f\"Drift flag saved to: {DRIFT_FLAG_PATH}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving drift flag: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run comprehensive drift detection.\"\"\"\n",
    "    print(\"Starting comprehensive drift detection...\")\n",
    "    \n",
    "    # Setup MLflow\n",
    "    with setup_mlflow():\n",
    "        try:\n",
    "            # Load and validate data\n",
    "            reference_df, new_df = load_and_validate_data(REFERENCE_DATA_PATH, NEW_DATA_PATH)\n",
    "            \n",
    "            # Identify feature types\n",
    "            numerical_features, categorical_features = identify_feature_types(reference_df)\n",
    "            \n",
    "            # Log feature information\n",
    "            mlflow.log_param(\"numerical_features\", str(numerical_features))\n",
    "            mlflow.log_param(\"categorical_features\", str(categorical_features))\n",
    "            mlflow.log_param(\"drift_threshold\", DRIFT_THRESHOLD)\n",
    "            mlflow.log_param(\"drift_methods\", str(DRIFT_METHODS))\n",
    "            \n",
    "            # Log basic statistics\n",
    "            print(\"Logging basic statistics...\")\n",
    "            log_basic_statistics(reference_df, new_df, prefix=\"basic_stats_\")\n",
    "            \n",
    "            # Track drift detection across multiple methods\n",
    "            drift_detected_overall = False\n",
    "            drift_results = {}\n",
    "            \n",
    "            # Generate reports for each drift detection method\n",
    "            for method in DRIFT_METHODS:\n",
    "                print(f\"Generating drift report using {method} method...\")\n",
    "                \n",
    "                try:\n",
    "                    # Generate drift report using the log_evidently_reports function\n",
    "                    drift_detected = log_evidently_reports(reference_df, new_df, method=method)\n",
    "                    drift_results[method] = drift_detected\n",
    "                    \n",
    "                    if drift_detected:\n",
    "                        drift_detected_overall = True\n",
    "                    \n",
    "                    print(f\"Drift detection using {method}: {'DETECTED' if drift_detected else 'NOT DETECTED'}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {method} method: {e}\")\n",
    "                    drift_results[method] = False\n",
    "            \n",
    "            # Log overall drift detection result\n",
    "            mlflow.log_metric(\"drift_detected_overall\", 1.0 if drift_detected_overall else 0.0)\n",
    "            \n",
    "            # Log individual method results\n",
    "            for method, detected in drift_results.items():\n",
    "                mlflow.log_metric(f\"drift_detected_{method}\", 1.0 if detected else 0.0)\n",
    "            \n",
    "            # Write final drift detection result to a local file (following your reference code pattern)\n",
    "            drift_info = {\n",
    "                \"drift_detected\": drift_detected_overall,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"drift_threshold\": DRIFT_THRESHOLD,\n",
    "                \"methods_used\": DRIFT_METHODS,\n",
    "                \"individual_results\": drift_results,\n",
    "                \"reference_data_shape\": reference_df.shape,\n",
    "                \"new_data_shape\": new_df.shape\n",
    "            }\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(DRIFT_FLAG_PATH), exist_ok=True)\n",
    "            \n",
    "            with open(DRIFT_FLAG_PATH, \"w\") as f:\n",
    "                json.dump(drift_info, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nDrift Detection Summary:\")\n",
    "            print(f\"Overall drift detected: {'YES' if drift_detected_overall else 'NO'}\")\n",
    "            print(f\"Individual method results: {drift_results}\")\n",
    "            print(f\"Reports saved and logged to MLflow\")\n",
    "            print(f\"Drift flag saved to: {DRIFT_FLAG_PATH}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main execution: {e}\")\n",
    "            mlflow.log_param(\"error\", str(e))\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a49a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
