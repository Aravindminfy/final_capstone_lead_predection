{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f4724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056e484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:46:50,139 - __main__ - INFO - Data loaded successfully from C:\\Users\\Minfy.DESKTOP-3E50D5N\\Music\\customer_lead\\data\\Lead Scoring.csv\n",
      "2025-07-17 14:46:50,140 - __main__ - INFO - Dataset shape: (9240, 37)\n",
      "2025-07-17 14:46:50,140 - __main__ - INFO - Dataset columns: ['Prospect ID', 'Lead Number', 'Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']...\n",
      "2025-07-17 14:46:50,141 - __main__ - INFO - Initializing preprocessing pipeline\n",
      "2025-07-17 14:46:50,144 - __main__ - INFO - Initialized LeadDataPreprocessor with output directory: preprocessed_output\n",
      "2025-07-17 14:46:50,145 - __main__ - INFO - Input data validation passed\n",
      "2025-07-17 14:46:50,145 - __main__ - INFO - Processing training data\n",
      "2025-07-17 14:46:50,146 - __main__ - INFO - Performing fit_transform operation\n",
      "2025-07-17 14:46:50,146 - __main__ - INFO - Starting preprocessing pipeline fitting\n",
      "2025-07-17 14:46:50,147 - __main__ - INFO - Preparing data for preprocessing\n",
      "2025-07-17 14:46:50,150 - __main__ - INFO - Data preparation complete:\n",
      "2025-07-17 14:46:50,151 - __main__ - INFO -   Initial shape: (9240, 37)\n",
      "2025-07-17 14:46:50,151 - __main__ - INFO -   Columns removed: 14\n",
      "2025-07-17 14:46:50,151 - __main__ - INFO -   Final shape: (9240, 23)\n",
      "2025-07-17 14:46:50,152 - __main__ - INFO - Feature availability summary:\n",
      "2025-07-17 14:46:50,152 - __main__ - INFO -   Numerical features: 5\n",
      "2025-07-17 14:46:50,153 - __main__ - INFO -   High importance categorical: 8\n",
      "2025-07-17 14:46:50,153 - __main__ - INFO -   Medium importance categorical: 7\n",
      "2025-07-17 14:46:50,154 - __main__ - INFO -   Binary features: 3\n",
      "2025-07-17 14:46:50,154 - __main__ - INFO -   Features to drop: 0\n",
      "2025-07-17 14:46:50,155 - __main__ - INFO - Creating numerical pipeline for 5 features\n",
      "2025-07-17 14:46:50,155 - __main__ - INFO - Creating high-importance categorical pipeline for 8 features\n",
      "2025-07-17 14:46:50,155 - __main__ - INFO - Creating medium-importance categorical pipeline for 7 features\n",
      "2025-07-17 14:46:50,156 - __main__ - INFO - Creating binary pipeline for 3 features\n",
      "2025-07-17 14:46:50,156 - __main__ - INFO - Combining 4 preprocessing pipelines\n",
      "2025-07-17 14:46:50,157 - __main__ - INFO - Fitting complete preprocessing pipeline\n",
      "2025-07-17 14:46:50,195 - __main__ - INFO - Fitting MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,202 - __main__ - INFO - Transforming data with MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,222 - __main__ - INFO - Fitting MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,225 - __main__ - INFO - Transforming data with MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,260 - __main__ - INFO - Generated 109 feature names\n",
      "2025-07-17 14:46:50,261 - __main__ - INFO - Pipeline fitting complete. Generated 109 output features\n",
      "2025-07-17 14:46:50,262 - __main__ - INFO - Starting data transformation\n",
      "2025-07-17 14:46:50,262 - __main__ - INFO - Preparing data for preprocessing\n",
      "2025-07-17 14:46:50,266 - __main__ - INFO - Data preparation complete:\n",
      "2025-07-17 14:46:50,266 - __main__ - INFO -   Initial shape: (9240, 37)\n",
      "2025-07-17 14:46:50,267 - __main__ - INFO -   Columns removed: 3\n",
      "2025-07-17 14:46:50,268 - __main__ - INFO -   Final shape: (9240, 34)\n",
      "2025-07-17 14:46:50,268 - __main__ - INFO - Applying preprocessing transformations\n",
      "2025-07-17 14:46:50,293 - __main__ - INFO - Transforming data with MultiColumnLabelEncoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LEAD DATA PREPROCESSING PIPELINE - PRODUCTION VERSION\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:46:50,311 - __main__ - INFO - Transforming data with MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,336 - __main__ - INFO - Transformation complete:\n",
      "2025-07-17 14:46:50,337 - __main__ - INFO -   Original shape: (9240, 37)\n",
      "2025-07-17 14:46:50,337 - __main__ - INFO -   Final shape: (9240, 109)\n",
      "2025-07-17 14:46:50,338 - __main__ - INFO -   Features generated: 109\n",
      "2025-07-17 14:46:50,345 - __main__ - INFO -   Numerical features scaled to range: [0.000, 1.000]\n",
      "2025-07-17 14:46:50,591 - __main__ - INFO - Processed data saved to: preprocessed_output\\data\\lead_scoring_processed.csv\n",
      "2025-07-17 14:46:50,593 - __main__ - INFO - Training data processing completed successfully\n",
      "2025-07-17 14:46:50,597 - __main__ - INFO - Pipeline saved to: preprocessed_output\\models\\lead_scoring_pipeline_v1.pkl\n",
      "2025-07-17 14:46:50,598 - __main__ - INFO - Pipeline saved for production use: preprocessed_output\\models\\lead_scoring_pipeline_v1.pkl\n",
      "2025-07-17 14:46:50,600 - __main__ - INFO - Feature documentation saved to: preprocessed_output\\logs\\feature_documentation.csv\n",
      "2025-07-17 14:46:50,601 - __main__ - INFO - Feature documentation generated\n",
      "2025-07-17 14:46:50,602 - __main__ - ERROR - Failed to save pipeline or documentation: 'charmap' codec can't encode characters in position 2601-2603: character maps to <undefined>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lead Data Preprocessing Pipeline - Production Version\n",
    "\n",
    "This module provides a comprehensive data preprocessing pipeline specifically designed \n",
    "for lead scoring datasets. It handles missing values, feature encoding, scaling, and \n",
    "maintains consistency between training and inference phases.\n",
    "\n",
    "Key Features:\n",
    "- Robust handling of categorical and numerical features\n",
    "- Consistent preprocessing for training and inference\n",
    "- Automatic feature importance-based processing strategies\n",
    "- Comprehensive logging and error handling\n",
    "- Serializable pipeline for production deployment\n",
    "\n",
    "Author: Data Science Team\n",
    "Version: 1.0.0\n",
    "Dependencies: pandas, numpy, scikit-learn, joblib\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "\n",
    "# Configure warnings to avoid sklearn deprecation messages in production\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Configure logging for production monitoring\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('lead_preprocessor.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom Label Encoder that can handle multiple columns simultaneously and \n",
    "    gracefully manages unseen categories during inference.\n",
    "    \n",
    "    This transformer is designed to be used in scikit-learn pipelines and \n",
    "    maintains consistency between training and inference phases.\n",
    "    \n",
    "    Design Decisions:\n",
    "    - Inherits from BaseEstimator and TransformerMixin for sklearn compatibility\n",
    "    - Handles unseen categories by mapping them to the most frequent training category\n",
    "    - Stores individual encoders for each column to maintain flexibility\n",
    "    - Converts all inputs to strings to handle mixed data types consistently\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the multi-column label encoder.\"\"\"\n",
    "        self.encoders: Dict[Union[str, int], LabelEncoder] = {}\n",
    "        self.column_names: Optional[List[str]] = None\n",
    "        \n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Optional[np.ndarray] = None) -> 'MultiColumnLabelEncoder':\n",
    "        \"\"\"\n",
    "        Fit label encoders for each column in the input data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame or ndarray\n",
    "            Input data containing categorical features to encode\n",
    "        y : array-like, optional\n",
    "            Target values (ignored, present for sklearn compatibility)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MultiColumnLabelEncoder\n",
    "            Returns self for method chaining\n",
    "            \n",
    "        Notes:\n",
    "        ------\n",
    "        - Each column gets its own LabelEncoder to maintain independence\n",
    "        - String conversion ensures consistent handling of mixed data types\n",
    "        - Stores column names for DataFrame inputs to maintain consistency\n",
    "        \"\"\"\n",
    "        logger.info(\"Fitting MultiColumnLabelEncoder\")\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.column_names = X.columns.tolist()\n",
    "            for col in X.columns:\n",
    "                encoder = LabelEncoder()\n",
    "                # Convert to string to handle mixed data types and NaN values\n",
    "                col_data = X[col].astype(str).fillna('missing_value')\n",
    "                encoder.fit(col_data)\n",
    "                self.encoders[col] = encoder\n",
    "                logger.debug(f\"Fitted encoder for column {col} with {len(encoder.classes_)} classes\")\n",
    "        else:\n",
    "            # Handle numpy array input\n",
    "            self.column_names = None\n",
    "            for i in range(X.shape[1]):\n",
    "                encoder = LabelEncoder()\n",
    "                col_data = pd.Series(X[:, i]).astype(str).fillna('missing_value')\n",
    "                encoder.fit(col_data)\n",
    "                self.encoders[i] = encoder\n",
    "                logger.debug(f\"Fitted encoder for column {i} with {len(encoder.classes_)} classes\")\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform data using fitted encoders with robust handling of unseen categories.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame or ndarray\n",
    "            Input data to transform\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Transformed data with categorical values encoded as integers\n",
    "            \n",
    "        Notes:\n",
    "        ------\n",
    "        - Unseen categories are mapped to the most frequent training category\n",
    "        - This approach maintains model stability during inference\n",
    "        - Warnings are logged for monitoring data drift\n",
    "        \"\"\"\n",
    "        if not self.encoders:\n",
    "            raise ValueError(\"Encoder must be fitted before transform. Call fit() first.\")\n",
    "            \n",
    "        logger.info(\"Transforming data with MultiColumnLabelEncoder\")\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            result = X.copy()\n",
    "            for col in X.columns:\n",
    "                if col in self.encoders:\n",
    "                    result[col] = self._transform_column(X[col], self.encoders[col], col)\n",
    "            return result.values\n",
    "        else:\n",
    "            # Handle numpy array input\n",
    "            result = X.copy()\n",
    "            for i in range(X.shape[1]):\n",
    "                if i in self.encoders:\n",
    "                    col_series = pd.Series(X[:, i])\n",
    "                    result[:, i] = self._transform_column(col_series, self.encoders[i], f\"column_{i}\")\n",
    "            return result\n",
    "    \n",
    "    def _transform_column(self, col_data: pd.Series, encoder: LabelEncoder, col_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform a single column with robust unseen category handling.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        col_data : Series\n",
    "            Column data to transform\n",
    "        encoder : LabelEncoder\n",
    "            Fitted encoder for this column\n",
    "        col_name : str\n",
    "            Column name for logging purposes\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Transformed column data\n",
    "            \n",
    "        Strategy for unseen categories:\n",
    "        - Map to the first class in the encoder (most frequent from training)\n",
    "        - Log warning for monitoring and potential retraining needs\n",
    "        - This maintains model stability while alerting to data drift\n",
    "        \"\"\"\n",
    "        col_str = col_data.astype(str).fillna('missing_value')\n",
    "        unique_vals = set(col_str)\n",
    "        known_vals = set(encoder.classes_)\n",
    "        unseen_vals = unique_vals - known_vals\n",
    "        \n",
    "        if unseen_vals:\n",
    "            logger.warning(f\"Unseen categories in {col_name}: {unseen_vals}\")\n",
    "            # Replace unseen values with the first known class (most frequent from training)\n",
    "            col_str_processed = col_str.copy()\n",
    "            for unseen_val in unseen_vals:\n",
    "                col_str_processed = col_str_processed.replace(unseen_val, encoder.classes_[0])\n",
    "            return encoder.transform(col_str_processed)\n",
    "        else:\n",
    "            return encoder.transform(col_str)\n",
    "    \n",
    "    def fit_transform(self, X: Union[pd.DataFrame, np.ndarray], y: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"Fit and transform in one step for convenience.\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "\n",
    "class LeadDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Production-ready preprocessing pipeline for lead scoring data.\n",
    "    \n",
    "    This class implements a comprehensive preprocessing strategy based on \n",
    "    feature importance analysis and business requirements. It ensures \n",
    "    consistent data transformation between training and inference phases.\n",
    "    \n",
    "    Design Philosophy:\n",
    "    - Feature importance drives processing strategy (one-hot vs label encoding)\n",
    "    - Robust handling of missing values and unseen categories\n",
    "    - Minimal information loss while maintaining model performance\n",
    "    - Production-ready with comprehensive logging and error handling\n",
    "    - Serializable for deployment consistency\n",
    "    \n",
    "    Processing Strategy:\n",
    "    1. High-importance categorical features: One-hot encoding to preserve all information\n",
    "    2. Medium-importance categorical features: Label encoding to reduce dimensionality\n",
    "    3. Numerical features: MinMax scaling for consistent feature ranges\n",
    "    4. Binary features: Label encoding for Yes/No values\n",
    "    5. Low-variance features: Automatic removal to reduce noise\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_dir: str = 'preprocessed_output',\n",
    "                 pipeline_name: str = 'lead_preprocessor_pipeline.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize the lead data preprocessor with configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_dir : str, default='preprocessed_output'\n",
    "            Directory to save processed files and pipeline artifacts\n",
    "        pipeline_name : str, default='lead_preprocessor_pipeline.pkl'\n",
    "            Name of the serialized pipeline file\n",
    "            \n",
    "        Design Decisions:\n",
    "        - Separate output directory for organized file management\n",
    "        - Configurable pipeline name for version control\n",
    "        - Feature categorization based on EDA and business importance\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.pipeline: Optional[ColumnTransformer] = None\n",
    "        self.feature_names_: Optional[List[str]] = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Create output directory structure\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.output_dir, 'data'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.output_dir, 'models'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.output_dir, 'logs'), exist_ok=True)\n",
    "        \n",
    "        # Feature categorization based on EDA and business importance\n",
    "        self._initialize_feature_categories()\n",
    "        \n",
    "        logger.info(f\"Initialized LeadDataPreprocessor with output directory: {self.output_dir}\")\n",
    "    \n",
    "    def _initialize_feature_categories(self):\n",
    "        \"\"\"\n",
    "        Initialize feature categories based on EDA insights and business requirements.\n",
    "        \n",
    "        Design Rationale:\n",
    "        - Numerical features: Direct predictors of engagement and behavior\n",
    "        - High-importance categorical: Critical for conversion prediction (one-hot encoded)\n",
    "        - Medium-importance categorical: Useful but not critical (label encoded to save space)\n",
    "        - Binary features: Simple Yes/No preferences\n",
    "        - Features to drop: No predictive power or 100% single value\n",
    "        \"\"\"\n",
    "        \n",
    "        # Numerical features: Direct engagement and behavioral metrics\n",
    "        # These are continuous variables that benefit from MinMax scaling\n",
    "        self.numerical_features = [\n",
    "            'TotalVisits',                    # User engagement frequency\n",
    "            'Total Time Spent on Website',   # User engagement depth\n",
    "            'Page Views Per Visit',          # User engagement intensity\n",
    "            'Asymmetrique Activity Score',   # Proprietary engagement metric\n",
    "            'Asymmetrique Profile Score'     # Proprietary profile quality metric\n",
    "        ]\n",
    "        \n",
    "        # High-importance categorical features (mutual information > 0.1)\n",
    "        # These get one-hot encoding to preserve all category information\n",
    "        # Rationale: High predictive power justifies increased dimensionality\n",
    "        self.high_importance_categorical = [\n",
    "            'Tags',                           # Highest importance (0.3746) - lead categorization\n",
    "            'Lead Quality',                   # Second highest (0.1898) - quality assessment\n",
    "            'Lead Profile',                   # Third highest (0.1245) - profile type\n",
    "            'What is your current occupation', # Fourth highest (0.0970) - professional context\n",
    "            'Last Activity',                  # Recent behavior indicator\n",
    "            'Last Notable Activity',          # Significant behavior indicator\n",
    "            'Lead Source',                    # Business critical - marketing channel\n",
    "            'Lead Origin'                     # Business critical - first touchpoint\n",
    "        ]\n",
    "        \n",
    "        # Medium-importance categorical features\n",
    "        # These get label encoding to balance information retention with dimensionality\n",
    "        # Rationale: Useful features but don't justify high dimensionality of one-hot encoding\n",
    "        self.medium_importance_categorical = [\n",
    "            'Specialization',                           # Educational preference\n",
    "            'City',                                     # Geographic segmentation\n",
    "            'How did you hear about X Education',       # Marketing attribution\n",
    "            'What matters most to you in choosing a course', # Decision factors\n",
    "            'Country',                                  # Geographic segmentation\n",
    "            'Asymmetrique Activity Index',              # Activity level categorization\n",
    "            'Asymmetrique Profile Index'                # Profile quality categorization\n",
    "        ]\n",
    "        \n",
    "        # Binary features: Simple Yes/No preferences\n",
    "        # These get label encoding: Yes=1, No=0\n",
    "        self.binary_features = [\n",
    "            'Do Not Email',                    # Communication preference\n",
    "            'Do Not Call',                     # Communication preference\n",
    "            'A free copy of Mastering The Interview'  # Content engagement indicator\n",
    "        ]\n",
    "        \n",
    "        # System columns to exclude from feature processing\n",
    "        self.id_columns = ['Prospect ID', 'Lead Number']  # Unique identifiers\n",
    "        self.target_column = 'Converted'                  # Target variable\n",
    "        \n",
    "        # Features to drop based on EDA findings\n",
    "        # Rationale: >95% single value or no predictive power\n",
    "        self.features_to_drop = [\n",
    "            'Magazine',                               # 100% \"No\" - no variance\n",
    "            'Receive More Updates About Our Courses', # 100% \"No\" - no variance\n",
    "            'Update me on Supply Chain Content',      # 100% \"No\" - no variance\n",
    "            'Get updates on DM Content',              # 100% \"No\" - no variance\n",
    "            'I agree to pay the amount through cheque', # 100% \"No\" - no variance\n",
    "            'Search',                                 # 99.85% \"No\" - minimal variance\n",
    "            'Newspaper Article',                      # 99.98% \"No\" - minimal variance\n",
    "            'X Education Forums',                     # 99.99% \"No\" - minimal variance\n",
    "            'Newspaper',                              # 99.99% \"No\" - minimal variance\n",
    "            'Digital Advertisement',                  # 99.96% \"No\" - minimal variance\n",
    "            'Through Recommendations'                 # 99.92% \"No\" - minimal variance\n",
    "        ]\n",
    "    \n",
    "    def _validate_and_filter_features(self, X: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate and filter feature lists based on available columns in the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input dataset to validate against\n",
    "            \n",
    "        Purpose:\n",
    "        - Handles cases where expected features might be missing\n",
    "        - Provides clear feedback on feature availability\n",
    "        - Prevents runtime errors from missing columns\n",
    "        \"\"\"\n",
    "        available_features = set(X.columns)\n",
    "        \n",
    "        # Filter each feature list to only include existing features\n",
    "        self.numerical_features = [f for f in self.numerical_features if f in available_features]\n",
    "        self.high_importance_categorical = [f for f in self.high_importance_categorical if f in available_features]\n",
    "        self.medium_importance_categorical = [f for f in self.medium_importance_categorical if f in available_features]\n",
    "        self.binary_features = [f for f in self.binary_features if f in available_features]\n",
    "        self.features_to_drop = [f for f in self.features_to_drop if f in available_features]\n",
    "        \n",
    "        # Log feature availability summary\n",
    "        logger.info(f\"Feature availability summary:\")\n",
    "        logger.info(f\"  Numerical features: {len(self.numerical_features)}\")\n",
    "        logger.info(f\"  High importance categorical: {len(self.high_importance_categorical)}\")\n",
    "        logger.info(f\"  Medium importance categorical: {len(self.medium_importance_categorical)}\")\n",
    "        logger.info(f\"  Binary features: {len(self.binary_features)}\")\n",
    "        logger.info(f\"  Features to drop: {len(self.features_to_drop)}\")\n",
    "    \n",
    "    def _prepare_data(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and prepare the dataset by removing non-feature columns.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Raw input dataset\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Cleaned dataset ready for feature processing\n",
    "            \n",
    "        Cleaning Strategy:\n",
    "        1. Remove ID columns (not predictive, cause overfitting)\n",
    "        2. Remove target column (for feature preparation)\n",
    "        3. Remove low-variance features (no predictive power)\n",
    "        \"\"\"\n",
    "        logger.info(\"Preparing data for preprocessing\")\n",
    "        \n",
    "        initial_shape = X.shape\n",
    "        \n",
    "        # Combine all columns to remove\n",
    "        columns_to_remove = self.id_columns + [self.target_column] + self.features_to_drop\n",
    "        \n",
    "        # Remove only existing columns to avoid KeyError\n",
    "        existing_columns_to_remove = [col for col in columns_to_remove if col in X.columns]\n",
    "        X_processed = X.drop(columns=existing_columns_to_remove, errors='ignore')\n",
    "        \n",
    "        logger.info(f\"Data preparation complete:\")\n",
    "        logger.info(f\"  Initial shape: {initial_shape}\")\n",
    "        logger.info(f\"  Columns removed: {len(existing_columns_to_remove)}\")\n",
    "        logger.info(f\"  Final shape: {X_processed.shape}\")\n",
    "        \n",
    "        return X_processed\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame) -> 'LeadDataPreprocessor':\n",
    "        \"\"\"\n",
    "        Fit the preprocessing pipeline on training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Training dataset\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : LeadDataPreprocessor\n",
    "            Returns self for method chaining\n",
    "            \n",
    "        Process:\n",
    "        1. Data preparation (remove unwanted columns)\n",
    "        2. Feature validation and filtering\n",
    "        3. Pipeline creation for each feature type\n",
    "        4. Pipeline combination and fitting\n",
    "        5. Feature name generation\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting preprocessing pipeline fitting\")\n",
    "        \n",
    "        # Step 1: Prepare data\n",
    "        X_processed = self._prepare_data(X)\n",
    "        \n",
    "        # Step 2: Validate and filter features\n",
    "        self._validate_and_filter_features(X_processed)\n",
    "        \n",
    "        # Step 3: Create preprocessing pipelines\n",
    "        preprocessor_steps = []\n",
    "        \n",
    "        # Numerical features pipeline\n",
    "        if self.numerical_features:\n",
    "            logger.info(f\"Creating numerical pipeline for {len(self.numerical_features)} features\")\n",
    "            numerical_pipeline = Pipeline([\n",
    "                # Use median imputation for robustness against outliers\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                # MinMax scaling ensures all features are in [0,1] range\n",
    "                # This prevents features with larger scales from dominating\n",
    "                ('scaler', MinMaxScaler(feature_range=(0, 1)))\n",
    "            ])\n",
    "            preprocessor_steps.append(('numerical', numerical_pipeline, self.numerical_features))\n",
    "        \n",
    "        # High-importance categorical features pipeline\n",
    "        if self.high_importance_categorical:\n",
    "            logger.info(f\"Creating high-importance categorical pipeline for {len(self.high_importance_categorical)} features\")\n",
    "            high_cat_pipeline = Pipeline([\n",
    "                # Most frequent imputation preserves distribution\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                # One-hot encoding preserves all category information\n",
    "                # drop='first' prevents multicollinearity\n",
    "                # handle_unknown='ignore' gracefully handles new categories\n",
    "                ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "            ])\n",
    "            preprocessor_steps.append(('high_categorical', high_cat_pipeline, self.high_importance_categorical))\n",
    "        \n",
    "        # Medium-importance categorical features pipeline\n",
    "        if self.medium_importance_categorical:\n",
    "            logger.info(f\"Creating medium-importance categorical pipeline for {len(self.medium_importance_categorical)} features\")\n",
    "            medium_cat_pipeline = Pipeline([\n",
    "                # 'Unknown' creates explicit missing category\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "                # Label encoding reduces dimensionality while preserving ordinal information\n",
    "                ('encoder', MultiColumnLabelEncoder())\n",
    "            ])\n",
    "            preprocessor_steps.append(('medium_categorical', medium_cat_pipeline, self.medium_importance_categorical))\n",
    "        \n",
    "        # Binary features pipeline\n",
    "        if self.binary_features:\n",
    "            logger.info(f\"Creating binary pipeline for {len(self.binary_features)} features\")\n",
    "            binary_pipeline = Pipeline([\n",
    "                # Default to 'No' for conservative assumption\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='No')),\n",
    "                # Label encoding: Yes=1, No=0\n",
    "                ('encoder', MultiColumnLabelEncoder())\n",
    "            ])\n",
    "            preprocessor_steps.append(('binary', binary_pipeline, self.binary_features))\n",
    "        \n",
    "        # Step 4: Combine all pipelines\n",
    "        logger.info(f\"Combining {len(preprocessor_steps)} preprocessing pipelines\")\n",
    "        self.pipeline = ColumnTransformer(\n",
    "            transformers=preprocessor_steps,\n",
    "            remainder='drop'  # Drop any remaining unspecified columns\n",
    "        )\n",
    "        \n",
    "        # Step 5: Fit the complete pipeline\n",
    "        logger.info(\"Fitting complete preprocessing pipeline\")\n",
    "        self.pipeline.fit(X_processed)\n",
    "        \n",
    "        # Step 6: Generate feature names\n",
    "        self._generate_feature_names()\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        logger.info(f\"Pipeline fitting complete. Generated {len(self.feature_names_)} output features\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _generate_feature_names(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate descriptive feature names for the transformed output.\n",
    "        \n",
    "        Purpose:\n",
    "        - Maintains interpretability of processed features\n",
    "        - Enables feature importance analysis\n",
    "        - Supports model debugging and validation\n",
    "        \"\"\"\n",
    "        self.feature_names_ = []\n",
    "        \n",
    "        for name, transformer, features in self.pipeline.transformers_:\n",
    "            if name == 'numerical':\n",
    "                # Numerical features keep original names\n",
    "                self.feature_names_.extend(features)\n",
    "                \n",
    "            elif name == 'high_categorical':\n",
    "                # One-hot encoded features get descriptive names\n",
    "                try:\n",
    "                    encoder = transformer.named_steps['encoder']\n",
    "                    if hasattr(encoder, 'get_feature_names_out'):\n",
    "                        encoded_features = encoder.get_feature_names_out(features)\n",
    "                        self.feature_names_.extend(encoded_features)\n",
    "                    else:\n",
    "                        # Fallback for older sklearn versions\n",
    "                        self.feature_names_.extend([f\"{feat}_onehot\" for feat in features])\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not generate feature names for high_categorical: {e}\")\n",
    "                    self.feature_names_.extend([f\"{feat}_onehot\" for feat in features])\n",
    "                    \n",
    "            elif name in ['medium_categorical', 'binary']:\n",
    "                # Label encoded features get descriptive suffix\n",
    "                self.feature_names_.extend([f\"{feat}_encoded\" for feat in features])\n",
    "        \n",
    "        logger.info(f\"Generated {len(self.feature_names_)} feature names\")\n",
    "    \n",
    "    def transform(self, \n",
    "                  X: pd.DataFrame, \n",
    "                  save_to_csv: bool = True, \n",
    "                  filename: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data to transform\n",
    "        save_to_csv : bool, default=True\n",
    "            Whether to save transformed data to CSV\n",
    "        filename : str, optional\n",
    "            Custom filename for the output CSV\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Transformed data ready for machine learning\n",
    "            \n",
    "        Process:\n",
    "        1. Validation checks\n",
    "        2. Data preparation\n",
    "        3. Pipeline transformation\n",
    "        4. Result formatting\n",
    "        5. Optional saving\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted before transform. Call fit() first.\")\n",
    "        \n",
    "        logger.info(\"Starting data transformation\")\n",
    "        \n",
    "        # Store original information for reporting\n",
    "        original_shape = X.shape\n",
    "        original_index = X.index.copy()\n",
    "        \n",
    "        # Step 1: Prepare data (same cleaning as in fit)\n",
    "        X_processed = self._prepare_data(X)\n",
    "        \n",
    "        # Step 2: Apply transformation\n",
    "        logger.info(\"Applying preprocessing transformations\")\n",
    "        X_transformed = self.pipeline.transform(X_processed)\n",
    "        \n",
    "        # Step 3: Create output DataFrame\n",
    "        X_final = pd.DataFrame(\n",
    "            X_transformed,\n",
    "            columns=self.feature_names_,\n",
    "            index=original_index\n",
    "        )\n",
    "        \n",
    "        # Step 4: Log transformation summary\n",
    "        logger.info(\"Transformation complete:\")\n",
    "        logger.info(f\"  Original shape: {original_shape}\")\n",
    "        logger.info(f\"  Final shape: {X_final.shape}\")\n",
    "        logger.info(f\"  Features generated: {len(self.feature_names_)}\")\n",
    "        \n",
    "        # Validate numerical feature scaling\n",
    "        if self.numerical_features:\n",
    "            num_cols = [col for col in X_final.columns if any(nf in col for nf in self.numerical_features)]\n",
    "            if num_cols:\n",
    "                min_val = X_final[num_cols].min().min()\n",
    "                max_val = X_final[num_cols].max().max()\n",
    "                logger.info(f\"  Numerical features scaled to range: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "        \n",
    "        # Step 5: Save results if requested\n",
    "        if save_to_csv:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f'processed_data_{timestamp}.csv'\n",
    "            \n",
    "            filepath = os.path.join(self.output_dir, 'data', filename)\n",
    "            X_final.to_csv(filepath, index=False)\n",
    "            logger.info(f\"Processed data saved to: {filepath}\")\n",
    "        \n",
    "        return X_final\n",
    "    \n",
    "    def fit_transform(self, \n",
    "                      X: pd.DataFrame, \n",
    "                      save_to_csv: bool = True, \n",
    "                      filename: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit the pipeline and transform data in one step.\n",
    "        \n",
    "        Typically used for training data processing.\n",
    "        \"\"\"\n",
    "        logger.info(\"Performing fit_transform operation\")\n",
    "        self.fit(X)\n",
    "        return self.transform(X, save_to_csv=save_to_csv, filename=filename)\n",
    "    \n",
    "    def save_pipeline(self, pipeline_filename: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Save the fitted pipeline to disk for production deployment.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pipeline_filename : str, optional\n",
    "            Custom filename for the pipeline\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Path to the saved pipeline file\n",
    "            \n",
    "        Purpose:\n",
    "        - Enables consistent preprocessing in production\n",
    "        - Supports model versioning and rollback\n",
    "        - Ensures training/inference consistency\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted before saving\")\n",
    "        \n",
    "        if pipeline_filename is None:\n",
    "            pipeline_filename = self.pipeline_name\n",
    "        \n",
    "        pipeline_path = os.path.join(self.output_dir, 'models', pipeline_filename)\n",
    "        \n",
    "        # Save complete preprocessor object\n",
    "        joblib.dump(self, pipeline_path)\n",
    "        \n",
    "        logger.info(f\"Pipeline saved to: {pipeline_path}\")\n",
    "        return pipeline_path\n",
    "    \n",
    "    @classmethod\n",
    "    def load_pipeline(cls, pipeline_path: str) -> 'LeadDataPreprocessor':\n",
    "        \"\"\"\n",
    "        Load a previously saved pipeline for inference.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pipeline_path : str\n",
    "            Path to the saved pipeline file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        LeadDataPreprocessor\n",
    "            Loaded preprocessor ready for inference\n",
    "            \n",
    "        Usage:\n",
    "        - Production inference\n",
    "        - Model validation\n",
    "        - Consistent preprocessing across environments\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading pipeline from: {pipeline_path}\")\n",
    "        \n",
    "        if not os.path.exists(pipeline_path):\n",
    "            raise FileNotFoundError(f\"Pipeline file not found: {pipeline_path}\")\n",
    "        \n",
    "        preprocessor = joblib.load(pipeline_path)\n",
    "        \n",
    "        if not isinstance(preprocessor, cls):\n",
    "            raise ValueError(f\"Loaded object is not a {cls.__name__}\")\n",
    "        \n",
    "        logger.info(\"Pipeline loaded successfully\")\n",
    "        logger.info(f\"Ready to process data with {len(preprocessor.feature_names_)} output features\")\n",
    "        \n",
    "        return preprocessor\n",
    "    \n",
    "    def save_feature_documentation(self, filename: str = 'feature_documentation.csv') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Save comprehensive documentation of feature processing for model governance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str, default='feature_documentation.csv'\n",
    "            Output filename for documentation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Feature documentation DataFrame\n",
    "            \n",
    "        Purpose:\n",
    "        - Model interpretability and debugging\n",
    "        - Compliance and audit requirements\n",
    "        - Feature engineering documentation\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted first\")\n",
    "        \n",
    "        feature_docs = []\n",
    "        \n",
    "        # Document numerical features\n",
    "        for feat in self.numerical_features:\n",
    "            feature_docs.append({\n",
    "                'original_feature': feat,\n",
    "                'output_feature': feat,\n",
    "                'feature_type': 'numerical',\n",
    "                'encoding_method': 'MinMaxScaler',\n",
    "                'importance_level': 'high',\n",
    "                'missing_value_strategy': 'median_imputation',\n",
    "                'scaling_range': '[0, 1]',\n",
    "                'description': 'Continuous variable scaled to unit range for consistent feature importance'\n",
    "            })\n",
    "        \n",
    "        # Document high-importance categorical features\n",
    "        for feat in self.high_importance_categorical:\n",
    "            feature_docs.append({\n",
    "                'original_feature': feat,\n",
    "                'output_feature': f\"{feat}_*\",\n",
    "                'feature_type': 'categorical',\n",
    "                'encoding_method': 'OneHotEncoder',\n",
    "                'importance_level': 'high',\n",
    "                'missing_value_strategy': 'most_frequent',\n",
    "                'scaling_range': '[0, 1]',\n",
    "                'description': 'High-importance categorical converted to binary columns preserving all information'\n",
    "            })\n",
    "        \n",
    "        # Document medium-importance categorical features\n",
    "        for feat in self.medium_importance_categorical:\n",
    "            feature_docs.append({\n",
    "                'original_feature': feat,\n",
    "                'output_feature': f\"{feat}_encoded\",\n",
    "                'feature_type': 'categorical',\n",
    "                'encoding_method': 'LabelEncoder',\n",
    "                'importance_level': 'medium',\n",
    "                'missing_value_strategy': 'unknown_category',\n",
    "                'scaling_range': '[0, n_classes-1]',\n",
    "                'description': 'Medium-importance categorical converted to ordinal integers for dimensionality reduction'\n",
    "            })\n",
    "        \n",
    "        # Document binary features\n",
    "        for feat in self.binary_features:\n",
    "            feature_docs.append({\n",
    "                'original_feature': feat,\n",
    "                'output_feature': f\"{feat}_encoded\",\n",
    "                'feature_type': 'binary',\n",
    "                'encoding_method': 'LabelEncoder',\n",
    "                'importance_level': 'low',\n",
    "                'missing_value_strategy': 'default_no',\n",
    "                'scaling_range': '[0, 1]',\n",
    "                'description': 'Binary feature encoded as 0/1 with conservative missing value handling'\n",
    "            })\n",
    "        \n",
    "        # Create and save documentation\n",
    "        df_docs = pd.DataFrame(feature_docs)\n",
    "        filepath = os.path.join(self.output_dir, 'logs', filename)\n",
    "        df_docs.to_csv(filepath, index=False)\n",
    "        \n",
    "        logger.info(f\"Feature documentation saved to: {filepath}\")\n",
    "        return df_docs\n",
    "    \n",
    "    def get_processing_summary(self) -> Dict[str, Union[int, str, bool]]:\n",
    "        \"\"\"\n",
    "        Get comprehensive summary of preprocessing configuration.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Summary of preprocessing configuration and status\n",
    "            \n",
    "        Purpose:\n",
    "        - Pipeline monitoring and validation\n",
    "        - Model governance and documentation\n",
    "        - Performance analysis\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return {\"status\": \"Pipeline not fitted\"}\n",
    "        \n",
    "        summary = {\n",
    "            'pipeline_status': 'fitted',\n",
    "            'total_output_features': len(self.feature_names_),\n",
    "            'numerical_features_count': len(self.numerical_features),\n",
    "            'high_importance_categorical_count': len(self.high_importance_categorical),\n",
    "            'medium_importance_categorical_count': len(self.medium_importance_categorical),\n",
    "            'binary_features_count': len(self.binary_features),\n",
    "            'features_dropped_count': len(self.features_to_drop),\n",
    "            'scaling_method': 'MinMaxScaler [0,1]',\n",
    "            'high_categorical_encoding': 'OneHotEncoder',\n",
    "            'medium_categorical_encoding': 'LabelEncoder',\n",
    "            'binary_encoding': 'LabelEncoder',\n",
    "            'missing_value_strategies': {\n",
    "                'numerical': 'median',\n",
    "                'high_categorical': 'most_frequent',\n",
    "                'medium_categorical': 'unknown',\n",
    "                'binary': 'no'\n",
    "            },\n",
    "            'output_directory': self.output_dir,\n",
    "            'pipeline_file': self.pipeline_name,\n",
    "            'is_production_ready': True\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def validate_input_data(self, X: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Validate input data for preprocessing compatibility.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data to validate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (is_valid, list_of_issues)\n",
    "            \n",
    "        Purpose:\n",
    "        - Early detection of data quality issues\n",
    "        - Prevents pipeline failures in production\n",
    "        - Provides actionable feedback for data fixes\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for empty dataset\n",
    "        if X.empty:\n",
    "            issues.append(\"Input dataset is empty\")\n",
    "        \n",
    "        # Check for minimum required columns\n",
    "        required_columns = (self.numerical_features + \n",
    "                          self.high_importance_categorical + \n",
    "                          self.medium_importance_categorical + \n",
    "                          self.binary_features)\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in X.columns]\n",
    "        if missing_columns:\n",
    "            issues.append(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        # Check for excessive missing values\n",
    "        for col in self.numerical_features:\n",
    "            if col in X.columns:\n",
    "                missing_pct = X[col].isnull().sum() / len(X)\n",
    "                if missing_pct > 0.8:  # 80% threshold\n",
    "                    issues.append(f\"Excessive missing values in {col}: {missing_pct:.1%}\")\n",
    "        \n",
    "        # Check data types\n",
    "        for col in self.numerical_features:\n",
    "            if col in X.columns:\n",
    "                if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "                    issues.append(f\"Non-numeric data in numerical column {col}\")\n",
    "        \n",
    "        is_valid = len(issues) == 0\n",
    "        \n",
    "        if not is_valid:\n",
    "            logger.warning(f\"Input data validation failed with {len(issues)} issues\")\n",
    "            for issue in issues:\n",
    "                logger.warning(f\"  - {issue}\")\n",
    "        else:\n",
    "            logger.info(\"Input data validation passed\")\n",
    "        \n",
    "        return is_valid, issues\n",
    "\n",
    "    def get_feature_importance_weights(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get feature importance weights based on processing strategy.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Feature importance weights for model interpretation\n",
    "            \n",
    "        Purpose:\n",
    "        - Provides weights for feature importance analysis\n",
    "        - Supports model interpretation and business insights\n",
    "        - Accounts for encoding-induced feature multiplication\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted first\")\n",
    "        \n",
    "        weights = {}\n",
    "        \n",
    "        # Numerical features get weight 1.0 (baseline)\n",
    "        for feat in self.numerical_features:\n",
    "            weights[feat] = 1.0\n",
    "        \n",
    "        # High-importance categorical features get higher weights\n",
    "        # but distributed across one-hot encoded columns\n",
    "        for feat in self.high_importance_categorical:\n",
    "            weights[feat] = 0.8  # Slightly lower due to one-hot distribution\n",
    "        \n",
    "        # Medium-importance categorical features get medium weights\n",
    "        for feat in self.medium_importance_categorical:\n",
    "            weights[feat] = 0.6\n",
    "        \n",
    "        # Binary features get lower weights\n",
    "        for feat in self.binary_features:\n",
    "            weights[feat] = 0.4\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def generate_preprocessing_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate comprehensive preprocessing report for documentation.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Detailed preprocessing report\n",
    "            \n",
    "        Purpose:\n",
    "        - Model governance and compliance\n",
    "        - Team communication and handoff\n",
    "        - Performance analysis and optimization\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return \"Pipeline not fitted - cannot generate report\"\n",
    "        \n",
    "        report_lines = []\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(\"LEAD DATA PREPROCESSING PIPELINE REPORT\")\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_lines.append(f\"Pipeline Status: {'FITTED' if self.is_fitted else 'NOT FITTED'}\")\n",
    "        report_lines.append(f\"Output Directory: {self.output_dir}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Feature Processing Summary\n",
    "        report_lines.append(\"FEATURE PROCESSING SUMMARY\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(f\"Total Output Features: {len(self.feature_names_)}\")\n",
    "        report_lines.append(f\"Numerical Features: {len(self.numerical_features)} -> MinMax Scaled [0,1]\")\n",
    "        report_lines.append(f\"High-Importance Categorical: {len(self.high_importance_categorical)} -> One-Hot Encoded\")\n",
    "        report_lines.append(f\"Medium-Importance Categorical: {len(self.medium_importance_categorical)} -> Label Encoded\")\n",
    "        report_lines.append(f\"Binary Features: {len(self.binary_features)} -> Label Encoded\")\n",
    "        report_lines.append(f\"Features Dropped: {len(self.features_to_drop)} (low variance)\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Processing Strategy Details\n",
    "        report_lines.append(\"PROCESSING STRATEGY DETAILS\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(\"1. Numerical Features:\")\n",
    "        report_lines.append(\"   - Missing Value Strategy: Median imputation (robust to outliers)\")\n",
    "        report_lines.append(\"   - Scaling: MinMax [0,1] (prevents feature domination)\")\n",
    "        for feat in self.numerical_features:\n",
    "            report_lines.append(f\"   - {feat}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"2. High-Importance Categorical Features:\")\n",
    "        report_lines.append(\"   - Missing Value Strategy: Most frequent category\")\n",
    "        report_lines.append(\"   - Encoding: One-Hot (preserves all information)\")\n",
    "        report_lines.append(\"   - Rationale: High predictive power justifies dimensionality increase\")\n",
    "        for feat in self.high_importance_categorical:\n",
    "            report_lines.append(f\"   - {feat}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"3. Medium-Importance Categorical Features:\")\n",
    "        report_lines.append(\"   - Missing Value Strategy: 'Unknown' category\")\n",
    "        report_lines.append(\"   - Encoding: Label Encoding (dimensionality reduction)\")\n",
    "        report_lines.append(\"   - Rationale: Balances information retention with efficiency\")\n",
    "        for feat in self.medium_importance_categorical:\n",
    "            report_lines.append(f\"   - {feat}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"4. Binary Features:\")\n",
    "        report_lines.append(\"   - Missing Value Strategy: Default to 'No' (conservative)\")\n",
    "        report_lines.append(\"   - Encoding: Label Encoding (Yes=1, No=0)\")\n",
    "        for feat in self.binary_features:\n",
    "            report_lines.append(f\"   - {feat}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Dropped Features\n",
    "        report_lines.append(\"5. Dropped Features (Low Variance):\")\n",
    "        report_lines.append(\"   - Rationale: >95% single value, no predictive power\")\n",
    "        for feat in self.features_to_drop:\n",
    "            report_lines.append(f\"   - {feat}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Production Considerations\n",
    "        report_lines.append(\"PRODUCTION CONSIDERATIONS\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(\"- Pipeline is serializable for consistent deployment\")\n",
    "        report_lines.append(\"- Handles unseen categories gracefully during inference\")\n",
    "        report_lines.append(\"- Comprehensive logging for monitoring and debugging\")\n",
    "        report_lines.append(\"- Validation checks prevent common data quality issues\")\n",
    "        report_lines.append(\"- Feature documentation supports model governance\")\n",
    "        report_lines.append(\"- Single processed data file for streamlined workflow\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # File Structure\n",
    "        report_lines.append(\"OUTPUT FILE STRUCTURE\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(\"preprocessed_output/\")\n",
    "        report_lines.append(\"├── data/\")\n",
    "        report_lines.append(\"│   └── lead_scoring_processed.csv\")\n",
    "        report_lines.append(\"├── models/\")\n",
    "        report_lines.append(\"│   └── lead_scoring_pipeline_v1.pkl\")\n",
    "        report_lines.append(\"└── logs/\")\n",
    "        report_lines.append(\"    ├── feature_documentation.csv\")\n",
    "        report_lines.append(\"    └── preprocessing_report.txt\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"=\" * 80)\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PRODUCTION USAGE EXAMPLE AND MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating production usage of the preprocessing pipeline.\n",
    "    \n",
    "    This function shows:\n",
    "    1. Data loading and validation\n",
    "    2. Pipeline fitting and training data processing\n",
    "    3. Pipeline serialization for deployment\n",
    "    4. New data processing with loaded pipeline\n",
    "    5. Documentation generation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure logging for main execution\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"LEAD DATA PREPROCESSING PIPELINE - PRODUCTION VERSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Load and validate data\n",
    "    try:\n",
    "        # Update this path to your actual data file\n",
    "        data_path = r'C:\\Users\\Minfy.DESKTOP-3E50D5N\\Music\\customer_lead\\data\\Lead Scoring.csv'  # Adjust path as needed\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            logger.error(f\"Data file not found: {data_path}\")\n",
    "            print(f\"Please ensure '{data_path}' exists in the current directory\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(data_path)\n",
    "        logger.info(f\"Data loaded successfully from {data_path}\")\n",
    "        logger.info(f\"Dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Basic data validation\n",
    "        if df.empty:\n",
    "            logger.error(\"Dataset is empty\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Dataset columns: {list(df.columns[:10])}...\")  # Show first 10\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Initialize and fit preprocessor\n",
    "    try:\n",
    "        logger.info(\"Initializing preprocessing pipeline\")\n",
    "        preprocessor = LeadDataPreprocessor(\n",
    "            output_dir='preprocessed_output',\n",
    "            pipeline_name='lead_scoring_pipeline_v1.pkl'\n",
    "        )\n",
    "        \n",
    "        # Validate input data\n",
    "        is_valid, issues = preprocessor.validate_input_data(df)\n",
    "        if not is_valid:\n",
    "            logger.warning(\"Data validation issues found, but continuing with processing\")\n",
    "        \n",
    "        # Fit and transform training data\n",
    "        logger.info(\"Processing training data\")\n",
    "        X_train_processed = preprocessor.fit_transform(\n",
    "            df,\n",
    "            save_to_csv=True,\n",
    "            filename='lead_scoring_processed.csv'  # Single file for processed data\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Training data processing completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process training data: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Save pipeline and generate documentation\n",
    "    try:\n",
    "        # Save pipeline for production deployment\n",
    "        pipeline_path = preprocessor.save_pipeline()\n",
    "        logger.info(f\"Pipeline saved for production use: {pipeline_path}\")\n",
    "        \n",
    "        # Generate feature documentation\n",
    "        feature_docs = preprocessor.save_feature_documentation()\n",
    "        logger.info(\"Feature documentation generated\")\n",
    "        \n",
    "        # Generate preprocessing report\n",
    "        report = preprocessor.generate_preprocessing_report()\n",
    "        report_path = os.path.join(preprocessor.output_dir, 'logs', 'preprocessing_report.txt')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        logger.info(f\"Preprocessing report saved: {report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save pipeline or documentation: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Demonstrate pipeline loading capability (without saving additional files)\n",
    "    try:\n",
    "        logger.info(\"Demonstrating pipeline loading capability\")\n",
    "        \n",
    "        # Load pipeline (simulating production environment)\n",
    "        loaded_preprocessor = LeadDataPreprocessor.load_pipeline(pipeline_path)\n",
    "        \n",
    "        # Validate loaded pipeline is ready for inference\n",
    "        logger.info(\"Pipeline loaded successfully and ready for inference\")\n",
    "        logger.info(f\"Loaded pipeline can process {len(loaded_preprocessor.feature_names_)} features\")\n",
    "        \n",
    "        # Test validation on sample data without saving\n",
    "        new_data_sample = df.sample(n=min(50, len(df)), random_state=42)\n",
    "        is_valid, issues = loaded_preprocessor.validate_input_data(new_data_sample)\n",
    "        \n",
    "        if is_valid:\n",
    "            logger.info(\"Sample data validation passed - pipeline ready for production inference\")\n",
    "        else:\n",
    "            logger.warning(f\"Sample data validation found {len(issues)} issues\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load or validate pipeline: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 5: Display final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PROCESSING COMPLETE - SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary = preprocessor.get_processing_summary()\n",
    "    print(\"\\nPipeline Configuration:\")\n",
    "    for key, value in summary.items():\n",
    "        if key != 'missing_value_strategies':\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    print(f\"\\nOutput Files Generated:\")\n",
    "    print(f\"  Data Files:\")\n",
    "    print(f\"    - preprocessed_output/data/lead_scoring_processed.csv\")\n",
    "    print(f\"  Model Files:\")\n",
    "    print(f\"    - preprocessed_output/models/lead_scoring_pipeline_v1.pkl\")\n",
    "    print(f\"  Documentation:\")\n",
    "    print(f\"    - preprocessed_output/logs/feature_documentation.csv\")\n",
    "    print(f\"    - preprocessed_output/logs/preprocessing_report.txt\")\n",
    "    print(f\"    - lead_preprocessor.log\")\n",
    "    \n",
    "    print(f\"\\nFinal Dataset Shape: {X_train_processed.shape}\")\n",
    "    print(f\"Features Generated: {len(preprocessor.feature_names_)}\")\n",
    "    print(f\"Ready for Machine Learning Model Training!\")\n",
    "    \n",
    "    # Display feature importance weights\n",
    "    print(f\"\\nFeature Importance Weights:\")\n",
    "    weights = preprocessor.get_feature_importance_weights()\n",
    "    for category, weight in [('numerical', 1.0), ('high_categorical', 0.8), \n",
    "                           ('medium_categorical', 0.6), ('binary', 0.4)]:\n",
    "        count = len([f for f, w in weights.items() if abs(w - weight) < 0.1])\n",
    "        if count > 0:\n",
    "            print(f\"  {category.replace('_', ' ').title()}: {weight} (n={count})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595c578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "facfb0fb",
   "metadata": {},
   "source": [
    "# testing the pipeline pkl file for the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b85f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:46:50,661 - __main__ - INFO - Starting data transformation\n",
      "2025-07-17 14:46:50,662 - __main__ - INFO - Preparing data for preprocessing\n",
      "2025-07-17 14:46:50,663 - __main__ - INFO - Data preparation complete:\n",
      "2025-07-17 14:46:50,664 - __main__ - INFO -   Initial shape: (50, 29)\n",
      "2025-07-17 14:46:50,666 - __main__ - INFO -   Columns removed: 3\n",
      "2025-07-17 14:46:50,666 - __main__ - INFO -   Final shape: (50, 26)\n",
      "2025-07-17 14:46:50,667 - __main__ - INFO - Applying preprocessing transformations\n",
      "2025-07-17 14:46:50,674 - __main__ - INFO - Transforming data with MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,675 - __main__ - WARNING - Unseen categories in column_0: {'HR Management'}\n",
      "2025-07-17 14:46:50,677 - __main__ - WARNING - Unseen categories in column_1: {'Delhi', 'Chennai', 'Bangalore', 'Kolkata'}\n",
      "2025-07-17 14:46:50,680 - __main__ - WARNING - Unseen categories in column_2: {'Word of Mouth', 'Advertisement'}\n",
      "2025-07-17 14:46:50,683 - __main__ - WARNING - Unseen categories in column_3: {'Price', 'Brand', 'Quality', 'Flexibility'}\n",
      "2025-07-17 14:46:50,685 - __main__ - WARNING - Unseen categories in column_4: {'UK', 'USA'}\n",
      "2025-07-17 14:46:50,688 - __main__ - INFO - Transforming data with MultiColumnLabelEncoder\n",
      "2025-07-17 14:46:50,692 - __main__ - INFO - Transformation complete:\n",
      "2025-07-17 14:46:50,693 - __main__ - INFO -   Original shape: (50, 29)\n",
      "2025-07-17 14:46:50,694 - __main__ - INFO -   Final shape: (50, 109)\n",
      "2025-07-17 14:46:50,694 - __main__ - INFO -   Features generated: 109\n",
      "2025-07-17 14:46:50,696 - __main__ - INFO -   Numerical features scaled to range: [-1.222, 9.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING SAVED PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "✓ Pipeline file found: preprocessed_output/models/lead_scoring_pipeline_v1.pkl\n",
      "✓ Pipeline file size: 0.02 MB\n",
      "Creating sample data...\n",
      "✓ Sample data created with 50 records\n",
      "\n",
      "Sample data info:\n",
      "  Shape: (50, 29)\n",
      "  Columns: 29\n",
      "  Missing values: 20\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing pipeline: preprocessed_output/models/lead_scoring_pipeline_v1.pkl\n",
      "Sample data shape: (50, 29)\n",
      "Loading saved pipeline...\n",
      "✓ Pipeline loaded successfully\n",
      "✓ Pipeline is fitted and ready to use\n",
      "✓ Expected output features: 109\n",
      "Performing basic input validation...\n",
      "✓ All expected columns are present\n",
      "Transforming sample data...\n",
      "✓ Processing successful!\n",
      "✓ Input shape: (50, 29)\n",
      "✓ Output shape: (50, 109)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Validating processed output...\n",
      "✓ Output is DataFrame: True\n",
      "✓ Same number of rows: True\n",
      "✓ No missing values: True\n",
      "✓ Numeric columns: 0\n",
      "✓ No infinite values: True\n",
      "✓ Feature names generated: 109 columns\n",
      "\n",
      "============================================================\n",
      "TEST RESULTS SUMMARY\n",
      "============================================================\n",
      "✅ Pipeline loading: SUCCESS\n",
      "✅ Data processing: SUCCESS\n",
      "✅ Output validation: SUCCESS\n",
      "\n",
      "Input Data:\n",
      "  Records: 50\n",
      "  Features: 29\n",
      "  Missing values: 20\n",
      "\n",
      "Output Data:\n",
      "  Records: 50\n",
      "  Features: 109\n",
      "  Missing values: 0\n",
      "\n",
      "🎉 Pipeline is working correctly and ready for production!\n",
      "\n",
      "Sample of processed data (first 3 rows, first 10 columns):\n",
      "  TotalVisits Total Time Spent on Website Page Views Per Visit  \\\n",
      "0    0.155378                    0.599912             0.023583   \n",
      "1    0.115538                    0.941461             0.164448   \n",
      "2    0.059761                    0.611796             0.154595   \n",
      "\n",
      "  Asymmetrique Activity Score Asymmetrique Profile Score Tags_Busy  \\\n",
      "0                    4.909091                   2.222222       0.0   \n",
      "1                    6.090909                   6.444444       0.0   \n",
      "2                    7.636364                   2.222222       0.0   \n",
      "\n",
      "  Tags_Closed by Horizzon Tags_Diploma holder (Not Eligible)  \\\n",
      "0                     0.0                                0.0   \n",
      "1                     0.0                                0.0   \n",
      "2                     0.0                                0.0   \n",
      "\n",
      "  Tags_Graduation in progress Tags_In confusion whether part time or DLP  \n",
      "0                         0.0                                        0.0  \n",
      "1                         0.0                                        0.0  \n",
      "2                         0.0                                        0.0  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple test script to verify the saved preprocessing pipeline works correctly.\n",
    "\n",
    "This script:\n",
    "1. Creates sample data similar to the original dataset\n",
    "2. Loads the saved pipeline using joblib directly\n",
    "3. Tests preprocessing on the sample data\n",
    "4. Validates the output\n",
    "\n",
    "Usage: Run this after the main pipeline has been trained and saved.\n",
    "Note: This script loads the pipeline directly using joblib, no module import needed.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_sample_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    Create sample data that mimics the structure of the original lead scoring dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int, default=100\n",
    "        Number of sample records to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Sample dataset with similar structure to original data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Create sample data with similar structure to original dataset\n",
    "    sample_data = {\n",
    "        # ID columns\n",
    "        'Prospect ID': [f'PROS_{i:05d}' for i in range(n_samples)],\n",
    "        'Lead Number': [f'LEAD_{i:05d}' for i in range(n_samples)],\n",
    "        \n",
    "        # Numerical features\n",
    "        'TotalVisits': np.random.randint(1, 50, n_samples),\n",
    "        'Total Time Spent on Website': np.random.randint(0, 3600, n_samples),\n",
    "        'Page Views Per Visit': np.random.uniform(1, 20, n_samples),\n",
    "        'Asymmetrique Activity Score': np.random.randint(0, 100, n_samples),\n",
    "        'Asymmetrique Profile Score': np.random.randint(0, 100, n_samples),\n",
    "        \n",
    "        # High importance categorical features\n",
    "        'Tags': np.random.choice(['Hot Lead', 'Cold Lead', 'Warm Lead', 'Lost Lead'], n_samples),\n",
    "        'Lead Quality': np.random.choice(['High', 'Medium', 'Low'], n_samples),\n",
    "        'Lead Profile': np.random.choice(['Potential Lead', 'Sure Lead', 'Not Sure'], n_samples),\n",
    "        'What is your current occupation': np.random.choice(['Student', 'Working Professional', 'Unemployed', 'Other'], n_samples),\n",
    "        'Last Activity': np.random.choice(['Email Opened', 'SMS Sent', 'Page Visited', 'Form Submitted'], n_samples),\n",
    "        'Last Notable Activity': np.random.choice(['Email Opened', 'SMS Sent', 'Page Visited', 'Form Submitted'], n_samples),\n",
    "        'Lead Source': np.random.choice(['Direct Traffic', 'Google', 'Organic Search', 'Reference'], n_samples),\n",
    "        'Lead Origin': np.random.choice(['Landing Page Submission', 'API', 'Quick Add Form'], n_samples),\n",
    "        \n",
    "        # Medium importance categorical features\n",
    "        'Specialization': np.random.choice(['Marketing Management', 'Finance Management', 'HR Management', 'Operations Management'], n_samples),\n",
    "        'City': np.random.choice(['Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata'], n_samples),\n",
    "        'How did you hear about X Education': np.random.choice(['Online Search', 'Social Media', 'Word of Mouth', 'Advertisement'], n_samples),\n",
    "        'What matters most to you in choosing a course': np.random.choice(['Flexibility', 'Quality', 'Price', 'Brand'], n_samples),\n",
    "        'Country': np.random.choice(['India', 'USA', 'UK', 'Australia'], n_samples),\n",
    "        'Asymmetrique Activity Index': np.random.choice(['01.High', '02.Medium', '03.Low'], n_samples),\n",
    "        'Asymmetrique Profile Index': np.random.choice(['01.High', '02.Medium', '03.Low'], n_samples),\n",
    "        \n",
    "        # Binary features\n",
    "        'Do Not Email': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'Do Not Call': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'A free copy of Mastering The Interview': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        \n",
    "        # Features that will be dropped (low variance)\n",
    "        'Magazine': ['No'] * n_samples,\n",
    "        'Search': ['No'] * n_samples,\n",
    "        'Newspaper': ['No'] * n_samples,\n",
    "        \n",
    "        # Target variable\n",
    "        'Converted': np.random.choice([0, 1], n_samples)\n",
    "    }\n",
    "    \n",
    "    # Add some missing values to test imputation\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Introduce missing values randomly\n",
    "    missing_columns = ['TotalVisits', 'City', 'Specialization', 'Do Not Email']\n",
    "    for col in missing_columns:\n",
    "        if col in df.columns:\n",
    "            # Make 10% of values missing\n",
    "            missing_indices = np.random.choice(df.index, size=int(0.1 * len(df)), replace=False)\n",
    "            df.loc[missing_indices, col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "def test_pipeline(pipeline_path, sample_data):\n",
    "    \"\"\"\n",
    "    Test the saved pipeline with sample data using direct joblib loading.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipeline_path : str\n",
    "        Path to the saved pipeline file\n",
    "    sample_data : DataFrame\n",
    "        Sample data to test with\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (success, processed_data, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Testing pipeline: {pipeline_path}\")\n",
    "        print(f\"Sample data shape: {sample_data.shape}\")\n",
    "        \n",
    "        # Load the saved pipeline directly using joblib\n",
    "        print(\"Loading saved pipeline...\")\n",
    "        loaded_preprocessor = joblib.load(pipeline_path)\n",
    "        \n",
    "        # Check if the loaded object has the expected attributes\n",
    "        if not hasattr(loaded_preprocessor, 'pipeline') or not hasattr(loaded_preprocessor, 'is_fitted'):\n",
    "            raise ValueError(\"Loaded object doesn't appear to be a valid LeadDataPreprocessor\")\n",
    "        \n",
    "        if not loaded_preprocessor.is_fitted:\n",
    "            raise ValueError(\"Loaded pipeline is not fitted\")\n",
    "        \n",
    "        print(f\"✓ Pipeline loaded successfully\")\n",
    "        print(f\"✓ Pipeline is fitted and ready to use\")\n",
    "        print(f\"✓ Expected output features: {len(loaded_preprocessor.feature_names_)}\")\n",
    "        \n",
    "        # Validate input data (basic checks)\n",
    "        print(\"Performing basic input validation...\")\n",
    "        \n",
    "        # Check for empty dataset\n",
    "        if sample_data.empty:\n",
    "            raise ValueError(\"Input dataset is empty\")\n",
    "        \n",
    "        # Check for minimum required columns (basic check)\n",
    "        required_columns = (loaded_preprocessor.numerical_features + \n",
    "                          loaded_preprocessor.high_importance_categorical + \n",
    "                          loaded_preprocessor.medium_importance_categorical + \n",
    "                          loaded_preprocessor.binary_features)\n",
    "        \n",
    "        available_columns = set(sample_data.columns)\n",
    "        missing_columns = [col for col in required_columns if col not in available_columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"⚠️  Warning: Missing some expected columns: {missing_columns[:5]}...\")\n",
    "            print(\"    Continuing with available columns...\")\n",
    "        else:\n",
    "            print(\"✓ All expected columns are present\")\n",
    "        \n",
    "        # Transform the sample data\n",
    "        print(\"Transforming sample data...\")\n",
    "        processed_data = loaded_preprocessor.transform(\n",
    "            sample_data, \n",
    "            save_to_csv=False  # Don't save during testing\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Processing successful!\")\n",
    "        print(f\"✓ Input shape: {sample_data.shape}\")\n",
    "        print(f\"✓ Output shape: {processed_data.shape}\")\n",
    "        \n",
    "        return True, processed_data, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline test failed: {str(e)}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        return False, None, error_msg\n",
    "\n",
    "def validate_output(processed_data, original_data):\n",
    "    \"\"\"\n",
    "    Validate the processed output data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_data : DataFrame\n",
    "        Processed data from pipeline\n",
    "    original_data : DataFrame\n",
    "        Original input data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if validation passes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nValidating processed output...\")\n",
    "        \n",
    "        # Check basic properties\n",
    "        print(f\"✓ Output is DataFrame: {isinstance(processed_data, pd.DataFrame)}\")\n",
    "        print(f\"✓ Same number of rows: {len(processed_data) == len(original_data)}\")\n",
    "        print(f\"✓ No missing values: {processed_data.isnull().sum().sum() == 0}\")\n",
    "        \n",
    "        # Check data types\n",
    "        numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
    "        print(f\"✓ Numeric columns: {len(numeric_cols)}\")\n",
    "        \n",
    "        # Check value ranges for scaled features\n",
    "        for col in numeric_cols:\n",
    "            min_val = processed_data[col].min()\n",
    "            max_val = processed_data[col].max()\n",
    "            print(f\"  {col}: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "        \n",
    "        # Check for any infinite or extremely large values\n",
    "        has_inf = np.isinf(processed_data.select_dtypes(include=[np.number])).any().any()\n",
    "        print(f\"✓ No infinite values: {not has_inf}\")\n",
    "        \n",
    "        # Check column names\n",
    "        print(f\"✓ Feature names generated: {len(processed_data.columns)} columns\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Output validation failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the pipeline test.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING SAVED PREPROCESSING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Define pipeline path\n",
    "    pipeline_path = \"preprocessed_output/models/lead_scoring_pipeline_v1.pkl\"\n",
    "    \n",
    "    # Check if pipeline file exists\n",
    "    if not os.path.exists(pipeline_path):\n",
    "        print(f\"❌ Pipeline file not found: {pipeline_path}\")\n",
    "        print(\"Please run the main preprocessing script first to create the pipeline.\")\n",
    "        print(\"Expected file location: preprocessed_output/models/lead_scoring_pipeline_v1.pkl\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✓ Pipeline file found: {pipeline_path}\")\n",
    "    \n",
    "    # Check file size to ensure it's not empty\n",
    "    file_size = os.path.getsize(pipeline_path)\n",
    "    print(f\"✓ Pipeline file size: {file_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Step 2: Create sample data\n",
    "    print(\"Creating sample data...\")\n",
    "    sample_data = create_sample_data(n_samples=50)\n",
    "    print(f\"✓ Sample data created with {len(sample_data)} records\")\n",
    "    \n",
    "    # Display sample data info\n",
    "    print(f\"\\nSample data info:\")\n",
    "    print(f\"  Shape: {sample_data.shape}\")\n",
    "    print(f\"  Columns: {len(sample_data.columns)}\")\n",
    "    print(f\"  Missing values: {sample_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Step 3: Test the pipeline\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    success, processed_data, error_msg = test_pipeline(pipeline_path, sample_data)\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"❌ Pipeline test failed!\")\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Validate output\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    validation_success = validate_output(processed_data, sample_data)\n",
    "    \n",
    "    if not validation_success:\n",
    "        print(\"❌ Output validation failed!\")\n",
    "        return\n",
    "    \n",
    "    # Step 5: Display results\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ Pipeline loading: SUCCESS\")\n",
    "    print(f\"✅ Data processing: SUCCESS\")\n",
    "    print(f\"✅ Output validation: SUCCESS\")\n",
    "    print(f\"\")\n",
    "    print(f\"Input Data:\")\n",
    "    print(f\"  Records: {len(sample_data)}\")\n",
    "    print(f\"  Features: {len(sample_data.columns)}\")\n",
    "    print(f\"  Missing values: {sample_data.isnull().sum().sum()}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Output Data:\")\n",
    "    print(f\"  Records: {len(processed_data)}\")\n",
    "    print(f\"  Features: {len(processed_data.columns)}\")\n",
    "    print(f\"  Missing values: {processed_data.isnull().sum().sum()}\")\n",
    "    print(f\"\")\n",
    "    print(f\"🎉 Pipeline is working correctly and ready for production!\")\n",
    "    \n",
    "    # Show sample of processed data\n",
    "    print(f\"\\nSample of processed data (first 3 rows, first 10 columns):\")\n",
    "    print(processed_data.iloc[:3, :10].round(3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1dd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f222b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f53b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lead",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
