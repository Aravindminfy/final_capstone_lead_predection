{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f808cf8",
   "metadata": {},
   "source": [
    "# Lead Data Preprocessing Pipeline Documentation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Architecture & Design Philosophy](#architecture--design-philosophy)\n",
    "3. [Core Components](#core-components)\n",
    "4. [Feature Engineering Strategy](#feature-engineering-strategy)\n",
    "5. [Data Processing Pipeline](#data-processing-pipeline)\n",
    "6. [Production Implementation](#production-implementation)\n",
    "7. [Usage Examples](#usage-examples)\n",
    "8. [Monitoring & Maintenance](#monitoring--maintenance)\n",
    "9. [API Reference](#api-reference)\n",
    "10. [Troubleshooting](#troubleshooting)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Lead Data Preprocessing Pipeline is a production-ready, comprehensive data preprocessing solution specifically designed for lead scoring datasets. It transforms raw lead data into machine learning-ready features while maintaining consistency between training and inference phases.\n",
    "\n",
    "### Key Features\n",
    "- **Robust Data Handling**: Graceful handling of missing values, unseen categories, and data drift\n",
    "- **Feature Importance-Driven**: Processing strategy based on feature importance analysis\n",
    "- **Production-Ready**: Serializable pipeline with comprehensive logging and monitoring\n",
    "- **Scalable Architecture**: Modular design supporting easy extension and maintenance\n",
    "- **Data Quality Validation**: Built-in validation checks for input data quality\n",
    "\n",
    "### Dependencies\n",
    "```python\n",
    "pandas >= 1.3.0\n",
    "numpy >= 1.21.0\n",
    "scikit-learn >= 1.0.0\n",
    "joblib >= 1.0.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture & Design Philosophy\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Consistency First**: Ensures identical processing between training and inference\n",
    "2. **Feature Importance-Driven**: Different encoding strategies based on feature importance\n",
    "3. **Minimal Information Loss**: Preserves critical information while reducing dimensionality\n",
    "4. **Graceful Degradation**: Handles unseen categories and missing values robustly\n",
    "5. **Production-Ready**: Comprehensive logging, serialization, and monitoring capabilities\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         LeadDataPreprocessor                               │\n",
    "├────────────────────────────────────────────────────────────────────────────┤\n",
    "│  ┌──────────────────────┐   ┌────────────────────────┐   ┌────────────────┐│\n",
    "│  │   Data Cleaning &    │   │ Feature Categorization │   │ Pipeline Setup ││\n",
    "│  │     Validation       │   │      & Validation      │   │   & Fitting    ││\n",
    "│  └──────────────────────┘   └────────────────────────┘   └────────────────┘│\n",
    "│               │                      │                        │             │\n",
    "│               ▼                      ▼                        ▼             │\n",
    "│     ┌────────────────┐     ┌────────────────┐      ┌────────────────────┐   │\n",
    "│     │ Numerical Data │     │ Categorical    │      │ Binary Variables   │   │\n",
    "│     │ Processing     │     │ Data Handling  │      │ Encoding           │   │\n",
    "│     │ (MinMaxScaler) │     │ (OneHot/Label) │      │ (LabelEncoder)     │   │\n",
    "│     └────────────────┘     └────────────────┘      └────────────────────┘   │\n",
    "│               │                      │                        │             │\n",
    "│               └──────────────┬───────┴──────────────┬────────┘             │\n",
    "│                              ▼                      ▼                      │\n",
    "│                    ┌────────────────────────────┐                          │\n",
    "│                    │     Combined Pipeline      │                          │\n",
    "│                    │  (via ColumnTransformer)   │                          │\n",
    "│                    └────────────────────────────┘                          │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### 1. MultiColumnLabelEncoder\n",
    "\n",
    "A custom transformer that extends sklearn's LabelEncoder to handle multiple columns simultaneously with robust unseen category handling.\n",
    "\n",
    "#### Key Features:\n",
    "- **Unseen Category Handling**: Maps unseen categories to the most frequent training category\n",
    "- **Missing Value Management**: Converts NaN values to explicit 'missing_value' category\n",
    "- **Sklearn Compatibility**: Inherits from BaseEstimator and TransformerMixin\n",
    "- **Type Consistency**: Converts all inputs to strings for consistent processing\n",
    "\n",
    "#### Implementation Details:\n",
    "```python\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Handles multiple categorical columns with graceful unseen category management.\n",
    "    \n",
    "    Strategy for unseen categories:\n",
    "    1. Map to most frequent training category (encoder.classes_[0])\n",
    "    2. Log warnings for monitoring data drift\n",
    "    3. Maintain model stability during inference\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "#### Processing Flow:\n",
    "1. **Fit Phase**: \n",
    "   - Create individual LabelEncoder for each column\n",
    "   - Convert all values to strings\n",
    "   - Handle missing values explicitly\n",
    "   - Store encoder classes for each column\n",
    "\n",
    "2. **Transform Phase**:\n",
    "   - Check for unseen categories\n",
    "   - Map unseen values to most frequent training category\n",
    "   - Log warnings for monitoring\n",
    "   - Apply transformation\n",
    "\n",
    "### 2. LeadDataPreprocessor\n",
    "\n",
    "The main preprocessing class that orchestrates the entire data transformation pipeline.\n",
    "\n",
    "#### Core Responsibilities:\n",
    "- **Feature Categorization**: Classifies features based on importance and data type\n",
    "- **Pipeline Management**: Creates and manages sklearn pipelines for each feature type\n",
    "- **Data Validation**: Validates input data quality and compatibility\n",
    "- **Serialization**: Saves/loads pipelines for production deployment\n",
    "- **Documentation**: Generates comprehensive processing documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering Strategy\n",
    "\n",
    "### Feature Categorization\n",
    "\n",
    "The pipeline categorizes features based on business importance and statistical analysis:\n",
    "\n",
    "#### 1. Numerical Features (MinMax Scaling)\n",
    "Features representing continuous engagement and behavioral metrics:\n",
    "\n",
    "| Feature | Description | Scaling Rationale |\n",
    "|---------|-------------|------------------|\n",
    "| `TotalVisits` | User engagement frequency | Prevents dominance of high-volume users |\n",
    "| `Total Time Spent on Website` | User engagement depth | Normalizes time-based metrics |\n",
    "| `Page Views Per Visit` | User engagement intensity | Standardizes behavioral patterns |\n",
    "| `Asymmetrique Activity Score` | Proprietary engagement metric | Ensures consistent feature importance |\n",
    "| `Asymmetrique Profile Score` | Proprietary profile quality | Maintains algorithmic fairness |\n",
    "\n",
    "**Processing Strategy:**\n",
    "- **Missing Values**: Median imputation (robust to outliers)\n",
    "- **Scaling**: MinMax [0,1] (prevents feature domination)\n",
    "- **Rationale**: Continuous variables benefit from normalization for ML algorithms\n",
    "\n",
    "#### 2. High-Importance Categorical Features (One-Hot Encoding)\n",
    "Features with high mutual information scores (>0.1) that significantly impact conversion:\n",
    "\n",
    "| Feature | Importance Score | Business Impact |\n",
    "|---------|-----------------|-----------------|\n",
    "| `Tags` | 0.3746 | Primary lead categorization |\n",
    "| `Lead Quality` | 0.1898 | Quality assessment metric |\n",
    "| `Lead Profile` | 0.1245 | Profile type classification |\n",
    "| `What is your current occupation` | 0.0970 | Professional context |\n",
    "| `Lead Source` | Business Critical | Marketing channel attribution |\n",
    "| `Lead Origin` | Business Critical | First touchpoint tracking |\n",
    "\n",
    "**Processing Strategy:**\n",
    "- **Missing Values**: Most frequent category (preserves distribution)\n",
    "- **Encoding**: One-Hot Encoding (preserves all category information)\n",
    "- **Rationale**: High predictive power justifies increased dimensionality\n",
    "\n",
    "#### 3. Medium-Importance Categorical Features (Label Encoding)\n",
    "Features with moderate importance that balance information retention with dimensionality:\n",
    "\n",
    "| Feature | Business Purpose | Encoding Rationale |\n",
    "|---------|------------------|-------------------|\n",
    "| `Specialization` | Educational preference | Ordinal relationship exists |\n",
    "| `City` | Geographic segmentation | Too many categories for one-hot |\n",
    "| `How did you hear about X Education` | Marketing attribution | Moderate importance |\n",
    "| `Country` | Geographic segmentation | Manageable category count |\n",
    "| `Asymmetrique Activity Index` | Activity categorization | Inherent ordering |\n",
    "\n",
    "**Processing Strategy:**\n",
    "- **Missing Values**: 'Unknown' category (explicit missing handling)\n",
    "- **Encoding**: Label Encoding (dimensionality reduction)\n",
    "- **Rationale**: Balances information retention with model efficiency\n",
    "\n",
    "#### 4. Binary Features (Label Encoding)\n",
    "Simple Yes/No preference features:\n",
    "\n",
    "| Feature | Default Value | Business Logic |\n",
    "|---------|---------------|----------------|\n",
    "| `Do Not Email` | No | Conservative communication assumption |\n",
    "| `Do Not Call` | No | Default to permissive contact |\n",
    "| `A free copy of Mastering The Interview` | No | Content engagement indicator |\n",
    "\n",
    "**Processing Strategy:**\n",
    "- **Missing Values**: Default to 'No' (conservative approach)\n",
    "- **Encoding**: Label Encoding (Yes=1, No=0)\n",
    "- **Rationale**: Simple binary encoding for boolean preferences\n",
    "\n",
    "#### 5. Features to Drop\n",
    "Features removed due to lack of predictive value:\n",
    "\n",
    "| Feature | Drop Reason | Single Value % |\n",
    "|---------|-------------|----------------|\n",
    "| `Magazine` | No variance | 100% \"No\" |\n",
    "| `Receive More Updates About Our Courses` | No variance | 100% \"No\" |\n",
    "| `Search` | Minimal variance | 99.85% \"No\" |\n",
    "| `Newspaper Article` | Minimal variance | 99.98% \"No\" |\n",
    "| `Digital Advertisement` | Minimal variance | 99.96% \"No\" |\n",
    "\n",
    "**Rationale**: Features with >95% single values provide no predictive information\n",
    "\n",
    "---\n",
    "\n",
    "## Data Processing Pipeline\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "The preprocessing pipeline uses sklearn's `ColumnTransformer` to apply different transformations to different feature types:\n",
    "\n",
    "```python\n",
    "ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('high_categorical', high_cat_pipeline, high_importance_categorical),\n",
    "    ('medium_categorical', medium_cat_pipeline, medium_importance_categorical),\n",
    "    ('binary', binary_pipeline, binary_features)\n",
    "])\n",
    "```\n",
    "\n",
    "### Processing Flow\n",
    "\n",
    "#### Phase 1: Data Preparation\n",
    "1. **Column Removal**: Remove ID columns, target variable, and low-variance features\n",
    "2. **Validation**: Check for required columns and data quality issues\n",
    "3. **Filtering**: Filter feature lists to only include available columns\n",
    "\n",
    "#### Phase 2: Feature Processing\n",
    "\n",
    "**Numerical Pipeline:**\n",
    "```python\n",
    "Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "```\n",
    "\n",
    "**High-Importance Categorical Pipeline:**\n",
    "```python\n",
    "Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "```\n",
    "\n",
    "**Medium-Importance Categorical Pipeline:**\n",
    "```python\n",
    "Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encoder', MultiColumnLabelEncoder())\n",
    "])\n",
    "```\n",
    "\n",
    "**Binary Pipeline:**\n",
    "```python\n",
    "Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='No')),\n",
    "    ('encoder', MultiColumnLabelEncoder())\n",
    "])\n",
    "```\n",
    "\n",
    "#### Phase 3: Output Generation\n",
    "1. **Feature Naming**: Generate descriptive names for transformed features\n",
    "2. **DataFrame Creation**: Create output DataFrame with proper column names\n",
    "3. **Validation**: Validate output ranges and data types\n",
    "4. **Serialization**: Save processed data and pipeline artifacts\n",
    "\n",
    "### Missing Value Strategy\n",
    "\n",
    "| Feature Type | Strategy | Rationale |\n",
    "|-------------|----------|-----------|\n",
    "| Numerical | Median imputation | Robust to outliers, preserves distribution |\n",
    "| High-Importance Categorical | Most frequent | Preserves training distribution |\n",
    "| Medium-Importance Categorical | 'Unknown' category | Explicit missing value handling |\n",
    "| Binary | Default to 'No' | Conservative business assumption |\n",
    "\n",
    "---\n",
    "\n",
    "## Production Implementation\n",
    "\n",
    "### Pipeline Serialization\n",
    "\n",
    "The pipeline supports full serialization for production deployment:\n",
    "\n",
    "```python\n",
    "# Save pipeline\n",
    "pipeline_path = preprocessor.save_pipeline('lead_pipeline_v1.pkl')\n",
    "\n",
    "# Load pipeline in production\n",
    "loaded_preprocessor = LeadDataPreprocessor.load_pipeline(pipeline_path)\n",
    "```\n",
    "\n",
    "### Data Quality Validation\n",
    "\n",
    "Built-in validation checks ensure data quality:\n",
    "\n",
    "```python\n",
    "def validate_input_data(self, X: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Comprehensive data validation including:\n",
    "    - Empty dataset check\n",
    "    - Missing column validation\n",
    "    - Excessive missing value detection\n",
    "    - Data type validation\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "### Monitoring & Logging\n",
    "\n",
    "Comprehensive logging for production monitoring:\n",
    "\n",
    "```python\n",
    "# Configure production logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('lead_preprocessor.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "Robust error handling for production stability:\n",
    "\n",
    "1. **Graceful Degradation**: Handles unseen categories without failing\n",
    "2. **Validation Checks**: Prevents common data quality issues\n",
    "3. **Informative Logging**: Provides actionable error messages\n",
    "4. **Fallback Strategies**: Default behaviors for edge cases\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Usage (Training)\n",
    "\n",
    "```python\n",
    "# Initialize preprocessor\n",
    "preprocessor = LeadDataPreprocessor(\n",
    "    output_dir='preprocessed_output',\n",
    "    pipeline_name='lead_pipeline_v1.pkl'\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "df_train = pd.read_csv('lead_training_data.csv')\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed = preprocessor.fit_transform(\n",
    "    df_train,\n",
    "    save_to_csv=True,\n",
    "    filename='training_data_processed.csv'\n",
    ")\n",
    "\n",
    "# Save pipeline for production\n",
    "pipeline_path = preprocessor.save_pipeline()\n",
    "```\n",
    "\n",
    "### Production Inference\n",
    "\n",
    "```python\n",
    "# Load trained pipeline\n",
    "preprocessor = LeadDataPreprocessor.load_pipeline('lead_pipeline_v1.pkl')\n",
    "\n",
    "# Load new data\n",
    "df_new = pd.read_csv('new_leads.csv')\n",
    "\n",
    "# Validate data quality\n",
    "is_valid, issues = preprocessor.validate_input_data(df_new)\n",
    "\n",
    "if is_valid:\n",
    "    # Transform new data\n",
    "    X_new_processed = preprocessor.transform(\n",
    "        df_new,\n",
    "        save_to_csv=True,\n",
    "        filename='new_leads_processed.csv'\n",
    "    )\n",
    "else:\n",
    "    print(\"Data quality issues:\", issues)\n",
    "```\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "```python\n",
    "# Process multiple files\n",
    "file_paths = ['batch1.csv', 'batch2.csv', 'batch3.csv']\n",
    "processed_files = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Validate before processing\n",
    "    is_valid, issues = preprocessor.validate_input_data(df)\n",
    "    \n",
    "    if is_valid:\n",
    "        X_processed = preprocessor.transform(\n",
    "            df,\n",
    "            save_to_csv=True,\n",
    "            filename=f'processed_{file_path}'\n",
    "        )\n",
    "        processed_files.append(f'processed_{file_path}')\n",
    "    else:\n",
    "        print(f\"Skipping {file_path} due to validation issues: {issues}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring & Maintenance\n",
    "\n",
    "### Key Metrics to Monitor\n",
    "\n",
    "1. **Data Drift Detection**:\n",
    "   - Monitor warnings for unseen categories\n",
    "   - Track missing value percentages\n",
    "   - Validate feature distributions\n",
    "\n",
    "2. **Pipeline Performance**:\n",
    "   - Processing time per batch\n",
    "   - Memory usage during transformation\n",
    "   - Error rates and types\n",
    "\n",
    "3. **Data Quality Metrics**:\n",
    "   - Validation failure rates\n",
    "   - Missing value trends\n",
    "   - Feature correlation changes\n",
    "\n",
    "### Maintenance Procedures\n",
    "\n",
    "#### Regular Model Retraining\n",
    "```python\n",
    "# Check for data drift indicators\n",
    "def check_data_drift(new_data, reference_stats):\n",
    "    \"\"\"\n",
    "    Compare new data statistics with reference statistics\n",
    "    from training data to detect drift\n",
    "    \"\"\"\n",
    "    drift_indicators = []\n",
    "    \n",
    "    # Check categorical distribution changes\n",
    "    for col in categorical_features:\n",
    "        new_dist = new_data[col].value_counts(normalize=True)\n",
    "        ref_dist = reference_stats[col]\n",
    "        \n",
    "        # Calculate distribution divergence\n",
    "        divergence = calculate_js_divergence(new_dist, ref_dist)\n",
    "        if divergence > 0.1:  # Threshold for significant drift\n",
    "            drift_indicators.append(f\"Distribution drift in {col}: {divergence}\")\n",
    "    \n",
    "    return drift_indicators\n",
    "```\n",
    "\n",
    "#### Pipeline Updates\n",
    "```python\n",
    "# Version control for pipeline updates\n",
    "def update_pipeline_version(current_version, new_version):\n",
    "    \"\"\"\n",
    "    Manage pipeline version updates with backward compatibility\n",
    "    \"\"\"\n",
    "    # Save current version as backup\n",
    "    backup_path = f'pipeline_backup_{current_version}.pkl'\n",
    "    \n",
    "    # Test new version on validation set\n",
    "    validation_results = validate_pipeline_performance(new_version)\n",
    "    \n",
    "    if validation_results['accuracy'] >= validation_results['baseline_accuracy']:\n",
    "        # Deploy new version\n",
    "        deploy_pipeline(new_version)\n",
    "        return True\n",
    "    else:\n",
    "        # Rollback to previous version\n",
    "        rollback_pipeline(current_version)\n",
    "        return False\n",
    "```\n",
    "\n",
    "### Troubleshooting Guide\n",
    "\n",
    "#### Common Issues and Solutions\n",
    "\n",
    "1. **Memory Issues with Large Datasets**:\n",
    "   ```python\n",
    "   # Process in chunks\n",
    "   def process_large_dataset(file_path, chunk_size=10000):\n",
    "       chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "       processed_chunks = []\n",
    "       \n",
    "       for chunk in chunks:\n",
    "           processed_chunk = preprocessor.transform(chunk, save_to_csv=False)\n",
    "           processed_chunks.append(processed_chunk)\n",
    "       \n",
    "       return pd.concat(processed_chunks, ignore_index=True)\n",
    "   ```\n",
    "\n",
    "2. **Handling New Categories**:\n",
    "   ```python\n",
    "   # Monitor and handle new categories\n",
    "   def handle_new_categories(new_data, feature_col):\n",
    "       \"\"\"\n",
    "       Detect and handle new categories in production data\n",
    "       \"\"\"\n",
    "       new_categories = set(new_data[feature_col].unique())\n",
    "       training_categories = set(preprocessor.encoders[feature_col].classes_)\n",
    "       \n",
    "       unseen_categories = new_categories - training_categories\n",
    "       \n",
    "       if unseen_categories:\n",
    "           # Log for monitoring\n",
    "           logger.warning(f\"New categories detected in {feature_col}: {unseen_categories}\")\n",
    "           \n",
    "           # Option 1: Map to most frequent category (current approach)\n",
    "           # Option 2: Trigger retraining pipeline\n",
    "           # Option 3: Create new category mapping\n",
    "   ```\n",
    "\n",
    "3. **Data Validation Failures**:\n",
    "   ```python\n",
    "   # Comprehensive data cleaning\n",
    "   def clean_problematic_data(df):\n",
    "       \"\"\"\n",
    "       Clean common data quality issues\n",
    "       \"\"\"\n",
    "       # Remove duplicate rows\n",
    "       df = df.drop_duplicates()\n",
    "       \n",
    "       # Handle extreme outliers in numerical features\n",
    "       for col in numerical_features:\n",
    "           Q1 = df[col].quantile(0.25)\n",
    "           Q3 = df[col].quantile(0.75)\n",
    "           IQR = Q3 - Q1\n",
    "           \n",
    "           # Cap outliers at 3*IQR\n",
    "           df[col] = df[col].clip(\n",
    "               lower=Q1 - 3*IQR,\n",
    "               upper=Q3 + 3*IQR\n",
    "           )\n",
    "       \n",
    "       return df\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## API Reference\n",
    "\n",
    "### LeadDataPreprocessor\n",
    "\n",
    "#### Constructor\n",
    "```python\n",
    "LeadDataPreprocessor(\n",
    "    output_dir: str = 'preprocessed_output',\n",
    "    pipeline_name: str = 'lead_preprocessor_pipeline.pkl'\n",
    ")\n",
    "```\n",
    "\n",
    "#### Methods\n",
    "\n",
    "##### `fit(X: pd.DataFrame) -> LeadDataPreprocessor`\n",
    "Fits the preprocessing pipeline on training data.\n",
    "\n",
    "**Parameters:**\n",
    "- `X`: Training dataset DataFrame\n",
    "\n",
    "**Returns:**\n",
    "- `self`: Fitted preprocessor instance\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "preprocessor = LeadDataPreprocessor()\n",
    "preprocessor.fit(training_data)\n",
    "```\n",
    "\n",
    "##### `transform(X: pd.DataFrame, save_to_csv: bool = True, filename: str = None) -> pd.DataFrame`\n",
    "Transforms data using the fitted pipeline.\n",
    "\n",
    "**Parameters:**\n",
    "- `X`: Input data to transform\n",
    "- `save_to_csv`: Whether to save transformed data\n",
    "- `filename`: Custom filename for output\n",
    "\n",
    "**Returns:**\n",
    "- `pd.DataFrame`: Transformed data\n",
    "\n",
    "##### `fit_transform(X: pd.DataFrame, save_to_csv: bool = True, filename: str = None) -> pd.DataFrame`\n",
    "Fits pipeline and transforms data in one step.\n",
    "\n",
    "##### `save_pipeline(pipeline_filename: str = None) -> str`\n",
    "Saves the fitted pipeline to disk.\n",
    "\n",
    "**Returns:**\n",
    "- `str`: Path to saved pipeline file\n",
    "\n",
    "##### `load_pipeline(pipeline_path: str) -> LeadDataPreprocessor`\n",
    "Class method to load a saved pipeline.\n",
    "\n",
    "**Parameters:**\n",
    "- `pipeline_path`: Path to saved pipeline\n",
    "\n",
    "**Returns:**\n",
    "- `LeadDataPreprocessor`: Loaded preprocessor instance\n",
    "\n",
    "##### `validate_input_data(X: pd.DataFrame) -> Tuple[bool, List[str]]`\n",
    "Validates input data for preprocessing compatibility.\n",
    "\n",
    "**Returns:**\n",
    "- `Tuple[bool, List[str]]`: (is_valid, list_of_issues)\n",
    "\n",
    "##### `get_processing_summary() -> Dict[str, Union[int, str, bool]]`\n",
    "Returns comprehensive summary of preprocessing configuration.\n",
    "\n",
    "##### `save_feature_documentation(filename: str = 'feature_documentation.csv') -> pd.DataFrame`\n",
    "Saves detailed feature processing documentation.\n",
    "\n",
    "##### `generate_preprocessing_report() -> str`\n",
    "Generates comprehensive preprocessing report for documentation.\n",
    "\n",
    "### MultiColumnLabelEncoder\n",
    "\n",
    "#### Constructor\n",
    "```python\n",
    "MultiColumnLabelEncoder()\n",
    "```\n",
    "\n",
    "#### Methods\n",
    "\n",
    "##### `fit(X: Union[pd.DataFrame, np.ndarray], y: np.ndarray = None) -> MultiColumnLabelEncoder`\n",
    "Fits label encoders for each column.\n",
    "\n",
    "##### `transform(X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray`\n",
    "Transforms data using fitted encoders.\n",
    "\n",
    "##### `fit_transform(X: Union[pd.DataFrame, np.ndarray], y: np.ndarray = None) -> np.ndarray`\n",
    "Fits and transforms in one step.\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "| Operation | Time Complexity | Space Complexity | Notes |\n",
    "|-----------|----------------|------------------|-------|\n",
    "| Fit | O(n × m) | O(m) | n=rows, m=features |\n",
    "| Transform | O(n × m) | O(n × m) | Output size depends on encoding |\n",
    "| One-Hot Encoding | O(n × k) | O(n × k × c) | k=categorical features, c=avg categories |\n",
    "| Label Encoding | O(n × k) | O(n × k) | More memory efficient |\n",
    "\n",
    "### Memory Optimization\n",
    "\n",
    "1. **Chunked Processing**: Process large datasets in chunks\n",
    "2. **Sparse Matrices**: Use sparse matrices for one-hot encoded features\n",
    "3. **Feature Selection**: Remove low-importance features to reduce memory\n",
    "4. **Dtype Optimization**: Use appropriate data types for memory efficiency\n",
    "\n",
    "### Scaling Considerations\n",
    "\n",
    "```python\n",
    "# Memory-efficient processing for large datasets\n",
    "def process_large_dataset(file_path, preprocessor, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Process large datasets in chunks to manage memory usage\n",
    "    \"\"\"\n",
    "    chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    output_path = 'processed_large_dataset.csv'\n",
    "    \n",
    "    header = True\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed_chunk = preprocessor.transform(chunk, save_to_csv=False)\n",
    "        \n",
    "        # Save chunk to file\n",
    "        processed_chunk.to_csv(\n",
    "            output_path,\n",
    "            mode='a',\n",
    "            header=header,\n",
    "            index=False\n",
    "        )\n",
    "        header = False  # Only write header for first chunk\n",
    "        \n",
    "        print(f\"Processed chunk {i+1}\")\n",
    "    \n",
    "    return output_path\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Security & Compliance\n",
    "\n",
    "### Data Privacy\n",
    "\n",
    "1. **No Data Persistence**: Raw data is not stored in pipeline objects\n",
    "2. **Secure Serialization**: Only processing parameters are serialized\n",
    "3. **Access Control**: Implement appropriate file permissions for pipeline files\n",
    "\n",
    "### Compliance Considerations\n",
    "\n",
    "1. **Data Lineage**: Full documentation of data transformations\n",
    "2. **Audit Trail**: Comprehensive logging of all processing steps\n",
    "3. **Reproducibility**: Deterministic processing with version control\n",
    "4. **Data Governance**: Clear documentation of feature engineering decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Development Workflow\n",
    "\n",
    "1. **Version Control**: Track pipeline versions and changes\n",
    "2. **Testing**: Comprehensive unit and integration testing\n",
    "3. **Documentation**: Maintain detailed documentation of changes\n",
    "4. **Validation**: Test on validation sets before production deployment\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "1. **Monitoring**: Implement comprehensive monitoring and alerting\n",
    "2. **Rollback Plan**: Maintain ability to rollback to previous versions\n",
    "3. **Performance Testing**: Regular performance benchmarking\n",
    "4. **Security**: Secure storage of pipeline artifacts and logs\n",
    "\n",
    "### Code Quality\n",
    "\n",
    "1. **Type Hints**: Use type hints for better code documentation\n",
    "2. **Error Handling**: Implement robust error handling and logging\n",
    "3. **Code Reviews**: Regular code reviews for quality assurance\n",
    "4. **Documentation**: Maintain up-to-date documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Lead Data Preprocessing Pipeline provides a robust, production-ready solution for transforming raw lead data into machine learning-ready features. Its feature importance-driven approach, comprehensive error handling, and extensive monitoring capabilities make it suitable for enterprise-scale deployment.\n",
    "\n",
    "Key advantages:\n",
    "- **Consistency**: Identical processing between training and inference\n",
    "- **Robustness**: Handles data quality issues and drift gracefully\n",
    "- **Scalability**: Supports large-scale data processing\n",
    "- **Maintainability**: Well-documented with comprehensive monitoring\n",
    "- **Flexibility**: Modular design supports easy extension and customization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d12dda",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
